nohup: ignoring input
fairseq plugins loaded...
2021-08-20 04:06:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 1000, 'log_format': None, 'tensorboard_logdir': '/home/v-weipeng/data1/Glat_xsum/save_tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'glat_plugins', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8192, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 8192, 'batch_size_valid': 8, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 5.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': 1e-09, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/v-weipeng/data1/Glat_xsum/save_models', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='glat', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, apply_bert_init=True, arch='glat', attention_dropout=0.0, azureml_logging=False, batch_size=32, batch_size_valid='8', best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=5.0, cpu=False, criterion='glat_loss', cross_self_attention=False, curriculum=0, data='/home/v-weipeng/data1/Xsum/processed_glat', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"iter_decode_max_iter": 0, "iter_decode_with_beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, left_pad_source=False, left_pad_target=False, length_loss_factor=0.05, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1000, max_target_positions=1000, max_tokens=8192, max_tokens_valid=8192, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, minus_p=0.2, model_parallel_size=1, mse_lambda=10, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, no_token_positional_embeddings=False, noise='full_mask', nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pred_length_offset=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/home/v-weipeng/data1/Glat_xsum/save_models', save_interval=1, save_interval_updates=0, scoring='bleu', seed=0, sentence_avg=False, sg_length_pred=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, src_embedding_copy=True, start_p=0.5, stop_min_lr=1e-09, stop_time_hours=0, suppress_crashes=False, target_lang=None, task='translation_lev_modified', tensorboard_logdir='/home/v-weipeng/data1/Glat_xsum/save_tensorboard', threshold_loss_scale=None, tokenizer=None, total_up=300000, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='glat_plugins', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation_lev_modified', 'data': '/home/v-weipeng/data1/Xsum/processed_glat', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': False, 'left_pad_target': False, 'max_source_positions': 1000, 'max_target_positions': 1000, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"iter_decode_max_iter": 0, "iter_decode_with_beam": 1}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False, 'noise': 'full_mask', 'start_p': 0.5, 'minus_p': 0.2, 'total_up': 300000}, 'criterion': Namespace(_name='glat_loss', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, apply_bert_init=True, arch='glat', attention_dropout=0.0, azureml_logging=False, batch_size=32, batch_size_valid='8', best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=5.0, cpu=False, criterion='glat_loss', cross_self_attention=False, curriculum=0, data='/home/v-weipeng/data1/Xsum/processed_glat', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"iter_decode_max_iter": 0, "iter_decode_with_beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, left_pad_source=False, left_pad_target=False, length_loss_factor=0.05, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1000, max_target_positions=1000, max_tokens=8192, max_tokens_valid=8192, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, minus_p=0.2, model_parallel_size=1, mse_lambda=10, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, no_token_positional_embeddings=False, noise='full_mask', nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pred_length_offset=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/home/v-weipeng/data1/Glat_xsum/save_models', save_interval=1, save_interval_updates=0, scoring='bleu', seed=0, sentence_avg=False, sg_length_pred=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, src_embedding_copy=True, start_p=0.5, stop_min_lr=1e-09, stop_time_hours=0, suppress_crashes=False, target_lang=None, task='translation_lev_modified', tensorboard_logdir='/home/v-weipeng/data1/Glat_xsum/save_tensorboard', threshold_loss_scale=None, tokenizer=None, total_up=300000, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='glat_plugins', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-08-20 04:06:37 | INFO | fairseq.tasks.translation | [src] dictionary: 30526 types
2021-08-20 04:06:37 | INFO | fairseq.tasks.translation | [tgt] dictionary: 30526 types
2021-08-20 04:06:37 | INFO | fairseq.data.data_utils | loaded 11,327 examples from: /home/v-weipeng/data1/Xsum/processed_glat/valid.src-tgt.src
2021-08-20 04:06:37 | INFO | fairseq.data.data_utils | loaded 11,327 examples from: /home/v-weipeng/data1/Xsum/processed_glat/valid.src-tgt.tgt
2021-08-20 04:06:37 | INFO | fairseq.tasks.translation | /home/v-weipeng/data1/Xsum/processed_glat valid src-tgt 11327 examples
2021-08-20 04:06:39 | INFO | fairseq_cli.train | Glat(
  (encoder): NATransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(30526, 512, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1002, 512, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): NATransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(30526, 512, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1002, 512, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=30526, bias=False)
    (embed_length): Embedding(1000, 512)
    (copy_attn): Linear(in_features=512, out_features=512, bias=False)
  )
)
2021-08-20 04:06:39 | INFO | fairseq_cli.train | task: TranslationLevenshteinModifiedTask
2021-08-20 04:06:39 | INFO | fairseq_cli.train | model: Glat
2021-08-20 04:06:39 | INFO | fairseq_cli.train | criterion: LabelSmoothedDualImitationCriterion
2021-08-20 04:06:39 | INFO | fairseq_cli.train | num. model params: 61,568,000 (num. trained: 61,568,000)
2021-08-20 04:06:43 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-08-20 04:06:43 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-08-20 04:06:43 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.copy_attn.bias
2021-08-20 04:06:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-08-20 04:06:43 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    
2021-08-20 04:06:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-08-20 04:06:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-08-20 04:06:43 | INFO | fairseq_cli.train | max tokens per GPU = 8192 and batch size per GPU = 32
2021-08-20 04:06:43 | INFO | fairseq.trainer | Preparing to load checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint_last.pt
2021-08-20 04:06:43 | INFO | fairseq.trainer | No existing checkpoint found /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint_last.pt
2021-08-20 04:06:43 | INFO | fairseq.trainer | loading train data for epoch 1
2021-08-20 04:06:43 | INFO | fairseq.data.data_utils | loaded 204,017 examples from: /home/v-weipeng/data1/Xsum/processed_glat/train.src-tgt.src
2021-08-20 04:06:43 | INFO | fairseq.data.data_utils | loaded 204,017 examples from: /home/v-weipeng/data1/Xsum/processed_glat/train.src-tgt.tgt
2021-08-20 04:06:43 | INFO | fairseq.tasks.translation | /home/v-weipeng/data1/Xsum/processed_glat train src-tgt 204017 examples
2021-08-20 04:06:43 | WARNING | fairseq.tasks.fairseq_task | 6,274 samples have invalid sizes and will be skipped, max_positions=(1000, 1000), first few sample ids=[79400, 28025, 127692, 146653, 203162, 127855, 144606, 140338, 8122, 2045]
2021-08-20 04:06:43 | INFO | fairseq.trainer | NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster
2021-08-20 04:06:43 | INFO | fairseq.trainer | begin training epoch 1
2021-08-20 04:06:43 | INFO | fairseq_cli.train | Start iterating over samples
/home/v-weipeng/.conda/envs/glat/lib/python3.7/site-packages/omegaconf/omegaconf.py:579: UserWarning: update() merge flag is is not specified, defaulting to False.
For more details, see https://github.com/omry/omegaconf/issues/367
  stacklevel=1,
2021-08-20 04:16:59 | INFO | train_inner | epoch 001:   1000 / 11278 loss=8.831, nll_loss=7.735, glat_accu=0, glat_context_p=0.5, word_ins=8.636, length=5.563, ppl=455.5, wps=666.4, ups=1.62, wpb=410.1, bsz=17.7, num_updates=1000, lr=0.000125075, gnorm=2.366, clip=7, loss_scale=128, train_wall=613, gb_free=14, wall=616
2021-08-20 04:27:40 | INFO | train_inner | epoch 001:   2000 / 11278 loss=7.866, nll_loss=6.601, glat_accu=0, glat_context_p=0.499, word_ins=7.702, length=4.599, ppl=233.24, wps=638.9, ups=1.56, wpb=409.4, bsz=17.9, num_updates=2000, lr=0.00025005, gnorm=1.836, clip=1, loss_scale=128, train_wall=639, gb_free=13, wall=1257
2021-08-20 04:38:23 | INFO | train_inner | epoch 001:   3000 / 11278 loss=8.301, nll_loss=7.096, glat_accu=0, glat_context_p=0.498, word_ins=8.144, length=4.578, ppl=315.3, wps=632.8, ups=1.56, wpb=406.6, bsz=17.5, num_updates=3000, lr=0.000375025, gnorm=1.983, clip=1.9, loss_scale=128, train_wall=640, gb_free=13.9, wall=1900
2021-08-20 04:49:04 | INFO | train_inner | epoch 001:   4000 / 11278 loss=8.528, nll_loss=7.36, glat_accu=0, glat_context_p=0.498, word_ins=8.376, length=4.55, ppl=369.05, wps=626.2, ups=1.56, wpb=401.6, bsz=17.4, num_updates=4000, lr=0.0005, gnorm=1.397, clip=0, loss_scale=128, train_wall=639, gb_free=13.3, wall=2541
2021-08-20 04:59:45 | INFO | train_inner | epoch 001:   5000 / 11278 loss=8.518, nll_loss=7.35, glat_accu=0, glat_context_p=0.497, word_ins=8.367, length=4.525, ppl=366.54, wps=626, ups=1.56, wpb=401.4, bsz=17.4, num_updates=5000, lr=0.000447214, gnorm=1.338, clip=0.1, loss_scale=128, train_wall=639, gb_free=12.6, wall=3182
2021-08-20 05:10:22 | INFO | train_inner | epoch 001:   6000 / 11278 loss=8.498, nll_loss=7.327, glat_accu=0, glat_context_p=0.496, word_ins=8.346, length=4.554, ppl=361.58, wps=638.8, ups=1.57, wpb=407.1, bsz=17.6, num_updates=6000, lr=0.000408248, gnorm=1.237, clip=0, loss_scale=128, train_wall=634, gb_free=13.5, wall=3819
2021-08-20 05:21:06 | INFO | train_inner | epoch 001:   7000 / 11278 loss=8.499, nll_loss=7.329, glat_accu=0, glat_context_p=0.496, word_ins=8.347, length=4.539, ppl=361.77, wps=631.8, ups=1.55, wpb=406.5, bsz=17.7, num_updates=7000, lr=0.000377964, gnorm=1.24, clip=0.3, loss_scale=128, train_wall=641, gb_free=13.1, wall=4463
2021-08-20 05:31:42 | INFO | train_inner | epoch 001:   8000 / 11278 loss=8.306, nll_loss=7.109, glat_accu=0, glat_context_p=0.495, word_ins=8.151, length=4.498, ppl=316.46, wps=634.2, ups=1.57, wpb=403.8, bsz=17.5, num_updates=8000, lr=0.000353553, gnorm=1.137, clip=0, loss_scale=128, train_wall=634, gb_free=13.1, wall=5099
2021-08-20 05:42:19 | INFO | train_inner | epoch 001:   9000 / 11278 loss=8.272, nll_loss=7.075, glat_accu=0, glat_context_p=0.494, word_ins=8.119, length=4.434, ppl=309.17, wps=626.3, ups=1.57, wpb=398.9, bsz=17.3, num_updates=9000, lr=0.000333333, gnorm=1.103, clip=0, loss_scale=128, train_wall=634, gb_free=14.3, wall=5736
2021-08-20 05:52:28 | INFO | train_inner | epoch 001:  10000 / 11278 loss=8.208, nll_loss=6.999, glat_accu=0, glat_context_p=0.494, word_ins=8.053, length=4.49, ppl=295.8, wps=661.5, ups=1.64, wpb=402.8, bsz=17.7, num_updates=10000, lr=0.000316228, gnorm=1.219, clip=0, loss_scale=128, train_wall=607, gb_free=13.2, wall=6345
2021-08-20 06:03:04 | INFO | train_inner | epoch 001:  11000 / 11278 loss=8.081, nll_loss=6.858, glat_accu=0, glat_context_p=0.493, word_ins=7.928, length=4.423, ppl=270.74, wps=614.2, ups=1.57, wpb=390.6, bsz=16.9, num_updates=11000, lr=0.000301511, gnorm=1.23, clip=0, loss_scale=128, train_wall=633, gb_free=13.2, wall=6981
2021-08-20 06:06:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 06:06:01 | WARNING | fairseq.tasks.fairseq_task | 339 samples have invalid sizes and will be skipped, max_positions=(1000, 1000), first few sample ids=[1517, 6945, 7595, 11288, 1787, 7435, 4537, 1406, 10040, 7583]
/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
2021-08-20 06:14:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.256 | nll_loss 6.917 | word_ins 8.037 | length 4.385 | ppl 305.72 | bleu 0 | wps 524.7 | wpb 184.6 | bsz 8 | num_updates 11278
2021-08-20 06:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 11278 updates
2021-08-20 06:14:05 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint1.pt
2021-08-20 06:14:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint1.pt
2021-08-20 06:14:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint1.pt (epoch 1 @ 11278 updates, score 0.0) (writing took 3.603967121045571 seconds)
2021-08-20 06:14:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-08-20 06:14:08 | INFO | train | epoch 001 | loss 8.346 | nll_loss 7.157 | glat_accu 0 | glat_context_p 0.496 | word_ins 8.188 | length 4.611 | ppl 325.41 | wps 595.6 | ups 1.48 | wpb 403.7 | bsz 17.5 | num_updates 11278 | lr 0.000297772 | gnorm 1.459 | clip 0.9 | loss_scale 128 | train_wall 7129 | gb_free 14.1 | wall 7645
2021-08-20 06:14:08 | INFO | fairseq.trainer | begin training epoch 2
2021-08-20 06:14:08 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 06:21:53 | INFO | train_inner | epoch 002:    722 / 11278 loss=7.963, nll_loss=6.719, glat_accu=0, glat_context_p=0.492, word_ins=7.805, length=4.512, ppl=249.49, wps=359.6, ups=0.89, wpb=406, bsz=17.7, num_updates=12000, lr=0.000288675, gnorm=1.389, clip=0, loss_scale=128, train_wall=639, gb_free=13.6, wall=8110
2021-08-20 06:32:31 | INFO | train_inner | epoch 002:   1722 / 11278 loss=7.912, nll_loss=6.661, glat_accu=0, glat_context_p=0.492, word_ins=7.754, length=4.472, ppl=240.81, wps=627.9, ups=1.57, wpb=400.2, bsz=17.4, num_updates=13000, lr=0.00027735, gnorm=1.534, clip=0.3, loss_scale=128, train_wall=635, gb_free=12.7, wall=8747
2021-08-20 06:43:10 | INFO | train_inner | epoch 002:   2722 / 11278 loss=7.858, nll_loss=6.601, glat_accu=0, glat_context_p=0.491, word_ins=7.701, length=4.43, ppl=231.96, wps=644.7, ups=1.56, wpb=412.2, bsz=17.8, num_updates=14000, lr=0.000267261, gnorm=1.453, clip=1.1, loss_scale=128, train_wall=637, gb_free=13.2, wall=9387
2021-08-20 06:53:43 | INFO | train_inner | epoch 002:   3722 / 11278 loss=7.805, nll_loss=6.54, glat_accu=0, glat_context_p=0.49, word_ins=7.647, length=4.464, ppl=223.69, wps=639.1, ups=1.58, wpb=404.4, bsz=17.7, num_updates=15000, lr=0.000258199, gnorm=1.52, clip=1.3, loss_scale=128, train_wall=630, gb_free=13.5, wall=10019
2021-08-20 07:04:17 | INFO | train_inner | epoch 002:   4722 / 11278 loss=7.803, nll_loss=6.539, glat_accu=0, glat_context_p=0.49, word_ins=7.645, length=4.451, ppl=223.37, wps=638.1, ups=1.58, wpb=404.7, bsz=17.5, num_updates=16000, lr=0.00025, gnorm=1.366, clip=2, loss_scale=128, train_wall=632, gb_free=12.6, wall=10653
2021-08-20 07:14:51 | INFO | train_inner | epoch 002:   5722 / 11278 loss=7.8, nll_loss=6.532, glat_accu=0, glat_context_p=0.489, word_ins=7.639, length=4.524, ppl=222.84, wps=624.8, ups=1.58, wpb=396.6, bsz=17.4, num_updates=17000, lr=0.000242536, gnorm=1.747, clip=1.3, loss_scale=256, train_wall=632, gb_free=12.7, wall=11288
2021-08-20 07:25:26 | INFO | train_inner | epoch 002:   6722 / 11278 loss=7.808, nll_loss=6.543, glat_accu=0, glat_context_p=0.488, word_ins=7.649, length=4.482, ppl=224.11, wps=635.7, ups=1.58, wpb=403.6, bsz=17.5, num_updates=18000, lr=0.000235702, gnorm=1.128, clip=0.6, loss_scale=256, train_wall=632, gb_free=13.4, wall=11923
2021-08-20 07:36:06 | INFO | train_inner | epoch 002:   7722 / 11278 loss=7.815, nll_loss=6.552, glat_accu=0, glat_context_p=0.488, word_ins=7.657, length=4.469, ppl=225.23, wps=634.8, ups=1.56, wpb=406, bsz=17.6, num_updates=19000, lr=0.000229416, gnorm=1.197, clip=1.1, loss_scale=256, train_wall=637, gb_free=13.8, wall=12563
2021-08-20 07:46:50 | INFO | train_inner | epoch 002:   8722 / 11278 loss=7.801, nll_loss=6.537, glat_accu=0, glat_context_p=0.487, word_ins=7.643, length=4.453, ppl=223.02, wps=636.9, ups=1.55, wpb=410.2, bsz=17.8, num_updates=20000, lr=0.000223607, gnorm=1.13, clip=0.6, loss_scale=256, train_wall=641, gb_free=13.4, wall=13207
2021-08-20 07:57:29 | INFO | train_inner | epoch 002:   9722 / 11278 loss=7.804, nll_loss=6.541, glat_accu=0, glat_context_p=0.486, word_ins=7.647, length=4.427, ppl=223.46, wps=622.8, ups=1.56, wpb=398.2, bsz=17.1, num_updates=21000, lr=0.000218218, gnorm=1.06, clip=0.3, loss_scale=256, train_wall=637, gb_free=13.6, wall=13846
2021-08-20 08:08:06 | INFO | train_inner | epoch 002:  10722 / 11278 loss=7.82, nll_loss=6.552, glat_accu=0, glat_context_p=0.486, word_ins=7.656, length=4.622, ppl=225.94, wps=630.5, ups=1.57, wpb=401.5, bsz=17.5, num_updates=22000, lr=0.000213201, gnorm=1.183, clip=1, loss_scale=256, train_wall=634, gb_free=13.2, wall=14483
2021-08-20 08:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 08:22:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.146 | nll_loss 6.796 | word_ins 7.926 | length 4.395 | ppl 283.17 | bleu 0 | wps 521.7 | wpb 184.6 | bsz 8 | num_updates 22556 | best_bleu 0
2021-08-20 08:22:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 22556 updates
2021-08-20 08:22:06 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint2.pt
2021-08-20 08:22:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint2.pt
2021-08-20 08:22:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint2.pt (epoch 2 @ 22556 updates, score 0.0) (writing took 20.476055230014026 seconds)
2021-08-20 08:22:27 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-08-20 08:22:27 | INFO | train | epoch 002 | loss 7.829 | nll_loss 6.568 | glat_accu 0 | glat_context_p 0.489 | word_ins 7.671 | length 4.479 | ppl 227.43 | wps 591.4 | ups 1.46 | wpb 403.7 | bsz 17.5 | num_updates 22556 | lr 0.000210557 | gnorm 1.316 | clip 0.9 | loss_scale 256 | train_wall 7162 | gb_free 13.7 | wall 15343
2021-08-20 08:22:27 | INFO | fairseq.trainer | begin training epoch 3
2021-08-20 08:22:27 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 08:27:13 | INFO | train_inner | epoch 003:    444 / 11278 loss=7.791, nll_loss=6.526, glat_accu=0, glat_context_p=0.485, word_ins=7.633, length=4.454, ppl=221.52, wps=348, ups=0.87, wpb=399.2, bsz=17.2, num_updates=23000, lr=0.000208514, gnorm=0.942, clip=0.1, loss_scale=256, train_wall=637, gb_free=13.4, wall=15630
2021-08-20 08:37:47 | INFO | train_inner | epoch 003:   1444 / 11278 loss=7.75, nll_loss=6.479, glat_accu=0, glat_context_p=0.484, word_ins=7.592, length=4.438, ppl=215.28, wps=637.7, ups=1.58, wpb=404.4, bsz=17.6, num_updates=24000, lr=0.000204124, gnorm=1.049, clip=0.4, loss_scale=256, train_wall=632, gb_free=13.5, wall=16264
2021-08-20 08:48:23 | INFO | train_inner | epoch 003:   2444 / 11278 loss=7.772, nll_loss=6.505, glat_accu=0, glat_context_p=0.484, word_ins=7.615, length=4.415, ppl=218.58, wps=636.3, ups=1.57, wpb=404.1, bsz=17.5, num_updates=25000, lr=0.0002, gnorm=0.944, clip=0.5, loss_scale=256, train_wall=633, gb_free=13.7, wall=16899
2021-08-20 08:58:51 | INFO | train_inner | epoch 003:   3444 / 11278 loss=7.776, nll_loss=6.506, glat_accu=0, glat_context_p=0.483, word_ins=7.616, length=4.529, ppl=219.23, wps=639.7, ups=1.59, wpb=401.8, bsz=17.4, num_updates=26000, lr=0.000196116, gnorm=0.945, clip=0.7, loss_scale=256, train_wall=626, gb_free=12.3, wall=17527
2021-08-20 09:09:33 | INFO | train_inner | epoch 003:   4444 / 11278 loss=7.766, nll_loss=6.5, glat_accu=0, glat_context_p=0.482, word_ins=7.61, length=4.378, ppl=217.62, wps=653.3, ups=1.56, wpb=420, bsz=18.2, num_updates=27000, lr=0.00019245, gnorm=0.966, clip=0.6, loss_scale=256, train_wall=640, gb_free=13.6, wall=18170
2021-08-20 09:20:09 | INFO | train_inner | epoch 003:   5444 / 11278 loss=7.778, nll_loss=6.512, glat_accu=0, glat_context_p=0.482, word_ins=7.62, length=4.428, ppl=219.45, wps=628, ups=1.57, wpb=399.1, bsz=17.3, num_updates=28000, lr=0.000188982, gnorm=0.918, clip=0.2, loss_scale=256, train_wall=633, gb_free=13.1, wall=18806
2021-08-20 09:30:47 | INFO | train_inner | epoch 003:   6444 / 11278 loss=7.774, nll_loss=6.509, glat_accu=0, glat_context_p=0.481, word_ins=7.618, length=4.395, ppl=218.89, wps=629.8, ups=1.57, wpb=402, bsz=17.3, num_updates=29000, lr=0.000185695, gnorm=0.886, clip=0, loss_scale=256, train_wall=636, gb_free=13.2, wall=19444
2021-08-20 09:41:17 | INFO | train_inner | epoch 003:   7444 / 11278 loss=7.757, nll_loss=6.486, glat_accu=0, glat_context_p=0.48, word_ins=7.598, length=4.459, ppl=216.3, wps=625.3, ups=1.59, wpb=393.8, bsz=17.2, num_updates=30000, lr=0.000182574, gnorm=0.96, clip=0.4, loss_scale=256, train_wall=627, gb_free=13.8, wall=20074
2021-08-20 09:51:55 | INFO | train_inner | epoch 003:   8444 / 11278 loss=7.773, nll_loss=6.506, glat_accu=0, glat_context_p=0.48, word_ins=7.615, length=4.434, ppl=218.69, wps=646.9, ups=1.57, wpb=412.5, bsz=18, num_updates=31000, lr=0.000179605, gnorm=0.919, clip=0.3, loss_scale=256, train_wall=635, gb_free=13.4, wall=20711
2021-08-20 10:02:31 | INFO | train_inner | epoch 003:   9444 / 11278 loss=7.759, nll_loss=6.486, glat_accu=0, glat_context_p=0.479, word_ins=7.597, length=4.536, ppl=216.62, wps=640.1, ups=1.57, wpb=407.3, bsz=17.8, num_updates=32000, lr=0.000176777, gnorm=0.944, clip=0.6, loss_scale=256, train_wall=634, gb_free=13.8, wall=21348
2021-08-20 10:13:08 | INFO | train_inner | epoch 003:  10444 / 11278 loss=7.759, nll_loss=6.489, glat_accu=0, glat_context_p=0.478, word_ins=7.6, length=4.462, ppl=216.56, wps=627.9, ups=1.57, wpb=399.8, bsz=17.5, num_updates=33000, lr=0.000174078, gnorm=0.828, clip=0, loss_scale=512, train_wall=634, gb_free=13.7, wall=21984
2021-08-20 10:21:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 10:30:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.995 | nll_loss 6.682 | word_ins 7.776 | length 4.385 | ppl 255.18 | bleu 0 | wps 520.8 | wpb 184.6 | bsz 8 | num_updates 33834 | best_bleu 0
2021-08-20 10:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 33834 updates
2021-08-20 10:30:05 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint3.pt
2021-08-20 10:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint3.pt
2021-08-20 10:30:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint3.pt (epoch 3 @ 33834 updates, score 0.0) (writing took 18.792482646007556 seconds)
2021-08-20 10:30:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-08-20 10:30:24 | INFO | train | epoch 003 | loss 7.768 | nll_loss 6.499 | glat_accu 0 | glat_context_p 0.481 | word_ins 7.61 | length 4.455 | ppl 217.97 | wps 593.1 | ups 1.47 | wpb 403.7 | bsz 17.5 | num_updates 33834 | lr 0.000171919 | gnorm 0.939 | clip 0.4 | loss_scale 512 | train_wall 7143 | gb_free 13.1 | wall 23021
2021-08-20 10:30:24 | INFO | fairseq.trainer | begin training epoch 4
2021-08-20 10:30:24 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 10:32:08 | INFO | train_inner | epoch 004:    166 / 11278 loss=7.765, nll_loss=6.493, glat_accu=0, glat_context_p=0.478, word_ins=7.604, length=4.532, ppl=217.56, wps=346, ups=0.88, wpb=394.6, bsz=17.1, num_updates=34000, lr=0.000171499, gnorm=0.944, clip=0.4, loss_scale=512, train_wall=632, gb_free=13.1, wall=23125
2021-08-20 10:42:52 | INFO | train_inner | epoch 004:   1166 / 11278 loss=7.77, nll_loss=6.5, glat_accu=0, glat_context_p=0.477, word_ins=7.61, length=4.473, ppl=218.25, wps=634, ups=1.55, wpb=408.2, bsz=17.6, num_updates=35000, lr=0.000169031, gnorm=0.869, clip=0.2, loss_scale=512, train_wall=641, gb_free=12.9, wall=23768
2021-08-20 10:53:30 | INFO | train_inner | epoch 004:   2166 / 11278 loss=7.784, nll_loss=6.514, glat_accu=0, glat_context_p=0.476, word_ins=7.623, length=4.514, ppl=220.34, wps=643, ups=1.57, wpb=410.4, bsz=17.8, num_updates=36000, lr=0.000166667, gnorm=0.815, clip=0, loss_scale=512, train_wall=636, gb_free=13.5, wall=24407
2021-08-20 11:04:10 | INFO | train_inner | epoch 004:   3166 / 11278 loss=7.75, nll_loss=6.48, glat_accu=0, glat_context_p=0.476, word_ins=7.593, length=4.405, ppl=215.28, wps=648.6, ups=1.56, wpb=414.8, bsz=18, num_updates=37000, lr=0.000164399, gnorm=0.858, clip=0.2, loss_scale=512, train_wall=637, gb_free=13.4, wall=25046
2021-08-20 11:14:46 | INFO | train_inner | epoch 004:   4166 / 11278 loss=7.754, nll_loss=6.483, glat_accu=0, glat_context_p=0.475, word_ins=7.595, length=4.464, ppl=215.94, wps=616.2, ups=1.57, wpb=392.4, bsz=17.1, num_updates=38000, lr=0.000162221, gnorm=0.849, clip=0.1, loss_scale=512, train_wall=634, gb_free=13.7, wall=25683
2021-08-20 11:25:25 | INFO | train_inner | epoch 004:   5166 / 11278 loss=7.766, nll_loss=6.495, glat_accu=0, glat_context_p=0.474, word_ins=7.606, length=4.474, ppl=217.62, wps=645.5, ups=1.57, wpb=412, bsz=17.8, num_updates=39000, lr=0.000160128, gnorm=0.817, clip=0, loss_scale=512, train_wall=636, gb_free=13.2, wall=26321
2021-08-20 11:36:03 | INFO | train_inner | epoch 004:   6166 / 11278 loss=7.744, nll_loss=6.473, glat_accu=0, glat_context_p=0.474, word_ins=7.586, length=4.416, ppl=214.34, wps=649.6, ups=1.57, wpb=414.6, bsz=18, num_updates=40000, lr=0.000158114, gnorm=0.828, clip=0.5, loss_scale=512, train_wall=636, gb_free=14, wall=26960
2021-08-20 11:46:40 | INFO | train_inner | epoch 004:   7166 / 11278 loss=7.752, nll_loss=6.477, glat_accu=0, glat_context_p=0.473, word_ins=7.59, length=4.522, ppl=215.53, wps=632.2, ups=1.57, wpb=402.7, bsz=17.6, num_updates=41000, lr=0.000156174, gnorm=0.845, clip=0.3, loss_scale=512, train_wall=634, gb_free=13.3, wall=27597
2021-08-20 11:56:54 | INFO | train_inner | epoch 004:   8166 / 11278 loss=7.754, nll_loss=6.483, glat_accu=0, glat_context_p=0.472, word_ins=7.595, length=4.444, ppl=215.93, wps=643.8, ups=1.63, wpb=395.1, bsz=17.1, num_updates=42000, lr=0.000154303, gnorm=0.898, clip=0.5, loss_scale=512, train_wall=611, gb_free=14, wall=28210
2021-08-20 12:07:23 | INFO | train_inner | epoch 004:   9166 / 11278 loss=7.765, nll_loss=6.496, glat_accu=0, glat_context_p=0.472, word_ins=7.607, length=4.421, ppl=217.51, wps=626.1, ups=1.59, wpb=394.2, bsz=17, num_updates=43000, lr=0.000152499, gnorm=0.786, clip=0, loss_scale=512, train_wall=627, gb_free=13.2, wall=28840
2021-08-20 12:17:58 | INFO | train_inner | epoch 004:  10166 / 11278 loss=7.761, nll_loss=6.492, glat_accu=0, glat_context_p=0.471, word_ins=7.603, length=4.413, ppl=216.9, wps=620.7, ups=1.58, wpb=394, bsz=17.1, num_updates=44000, lr=0.000150756, gnorm=0.865, clip=0.4, loss_scale=512, train_wall=632, gb_free=13.9, wall=29475
2021-08-20 12:28:34 | INFO | train_inner | epoch 004:  11166 / 11278 loss=7.745, nll_loss=6.473, glat_accu=0, glat_context_p=0.47, word_ins=7.586, length=4.43, ppl=214.57, wps=644.1, ups=1.57, wpb=409.8, bsz=17.9, num_updates=45000, lr=0.000149071, gnorm=0.768, clip=0, loss_scale=512, train_wall=634, gb_free=13, wall=30111
2021-08-20 12:29:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 12:37:52 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.102 | nll_loss 6.76 | word_ins 7.884 | length 4.373 | ppl 274.84 | bleu 0 | wps 522.2 | wpb 184.6 | bsz 8 | num_updates 45112 | best_bleu 0
2021-08-20 12:37:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 45112 updates
2021-08-20 12:37:52 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint4.pt
2021-08-20 12:37:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint4.pt
2021-08-20 12:38:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint4.pt (epoch 4 @ 45112 updates, score 0.0) (writing took 21.260692283976823 seconds)
2021-08-20 12:38:13 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-08-20 12:38:13 | INFO | train | epoch 004 | loss 7.758 | nll_loss 6.487 | glat_accu 0 | glat_context_p 0.474 | word_ins 7.599 | length 4.453 | ppl 216.44 | wps 593.7 | ups 1.47 | wpb 403.7 | bsz 17.5 | num_updates 45112 | lr 0.000148886 | gnorm 0.836 | clip 0.2 | loss_scale 512 | train_wall 7132 | gb_free 13.8 | wall 30690
2021-08-20 12:38:13 | INFO | fairseq.trainer | begin training epoch 5
2021-08-20 12:38:13 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 12:47:35 | INFO | train_inner | epoch 005:    888 / 11278 loss=7.714, nll_loss=6.439, glat_accu=0, glat_context_p=0.47, word_ins=7.556, length=4.389, ppl=210, wps=357.2, ups=0.88, wpb=407.6, bsz=17.8, num_updates=46000, lr=0.000147442, gnorm=0.778, clip=0, loss_scale=512, train_wall=631, gb_free=13.4, wall=31252
2021-08-20 12:58:12 | INFO | train_inner | epoch 005:   1888 / 11278 loss=7.767, nll_loss=6.494, glat_accu=0, glat_context_p=0.469, word_ins=7.605, length=4.499, ppl=217.8, wps=621.9, ups=1.57, wpb=395.9, bsz=17.2, num_updates=47000, lr=0.000145865, gnorm=0.849, clip=0.4, loss_scale=512, train_wall=634, gb_free=13.9, wall=31889
2021-08-20 13:08:52 | INFO | train_inner | epoch 005:   2888 / 11278 loss=7.754, nll_loss=6.481, glat_accu=0, glat_context_p=0.468, word_ins=7.594, length=4.435, ppl=215.8, wps=648.3, ups=1.56, wpb=414.7, bsz=18, num_updates=48000, lr=0.000144338, gnorm=0.76, clip=0, loss_scale=512, train_wall=636, gb_free=13.7, wall=32529
2021-08-20 13:19:26 | INFO | train_inner | epoch 005:   3888 / 11278 loss=7.74, nll_loss=6.467, glat_accu=0, glat_context_p=0.468, word_ins=7.582, length=4.401, ppl=213.78, wps=647.4, ups=1.58, wpb=410.3, bsz=17.9, num_updates=49000, lr=0.000142857, gnorm=0.82, clip=0.1, loss_scale=512, train_wall=631, gb_free=13.7, wall=33162
2021-08-20 13:30:02 | INFO | train_inner | epoch 005:   4888 / 11278 loss=7.747, nll_loss=6.474, glat_accu=0, glat_context_p=0.467, word_ins=7.587, length=4.442, ppl=214.84, wps=627.4, ups=1.57, wpb=399.5, bsz=17.4, num_updates=50000, lr=0.000141421, gnorm=0.798, clip=0.4, loss_scale=1024, train_wall=634, gb_free=13.7, wall=33799
2021-08-20 13:40:43 | INFO | train_inner | epoch 005:   5888 / 11278 loss=7.76, nll_loss=6.488, glat_accu=0, glat_context_p=0.466, word_ins=7.599, length=4.437, ppl=216.74, wps=614.7, ups=1.56, wpb=394.1, bsz=17.1, num_updates=51000, lr=0.000140028, gnorm=0.755, clip=0.1, loss_scale=1024, train_wall=639, gb_free=13.6, wall=34440
2021-08-20 13:51:19 | INFO | train_inner | epoch 005:   6888 / 11278 loss=7.744, nll_loss=6.468, glat_accu=0, glat_context_p=0.466, word_ins=7.583, length=4.449, ppl=214.3, wps=632.9, ups=1.57, wpb=402.1, bsz=17.6, num_updates=52000, lr=0.000138675, gnorm=0.774, clip=0.1, loss_scale=1024, train_wall=633, gb_free=13.8, wall=35076
2021-08-20 14:01:59 | INFO | train_inner | epoch 005:   7888 / 11278 loss=7.785, nll_loss=6.515, glat_accu=0, glat_context_p=0.465, word_ins=7.624, length=4.484, ppl=220.59, wps=625.4, ups=1.56, wpb=400.3, bsz=17.2, num_updates=53000, lr=0.000137361, gnorm=0.79, clip=0.3, loss_scale=1024, train_wall=638, gb_free=13.4, wall=35716
2021-08-20 14:12:35 | INFO | train_inner | epoch 005:   8888 / 11278 loss=7.728, nll_loss=6.45, glat_accu=0, glat_context_p=0.464, word_ins=7.566, length=4.486, ppl=212.04, wps=642, ups=1.57, wpb=408.1, bsz=17.6, num_updates=54000, lr=0.000136083, gnorm=0.756, clip=0, loss_scale=1024, train_wall=633, gb_free=13.2, wall=36351
2021-08-20 14:23:13 | INFO | train_inner | epoch 005:   9888 / 11278 loss=7.735, nll_loss=6.46, glat_accu=0, glat_context_p=0.464, word_ins=7.574, length=4.458, ppl=213.11, wps=636.7, ups=1.57, wpb=406.2, bsz=17.7, num_updates=55000, lr=0.00013484, gnorm=0.737, clip=0, loss_scale=1024, train_wall=635, gb_free=13.2, wall=36989
2021-08-20 14:33:50 | INFO | train_inner | epoch 005:  10888 / 11278 loss=7.782, nll_loss=6.511, glat_accu=0, glat_context_p=0.463, word_ins=7.62, length=4.48, ppl=220.06, wps=632.2, ups=1.57, wpb=403.2, bsz=17.4, num_updates=56000, lr=0.000133631, gnorm=0.762, clip=0, loss_scale=1024, train_wall=635, gb_free=13.8, wall=37627
2021-08-20 14:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 14:45:44 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.08 | nll_loss 6.777 | word_ins 7.862 | length 4.372 | ppl 270.65 | bleu 0 | wps 541.6 | wpb 184.6 | bsz 8 | num_updates 56390 | best_bleu 0
2021-08-20 14:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 56390 updates
2021-08-20 14:45:44 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint5.pt
2021-08-20 14:45:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint5.pt
2021-08-20 14:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint5.pt (epoch 5 @ 56390 updates, score 0.0) (writing took 18.142078000004403 seconds)
2021-08-20 14:46:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-08-20 14:46:03 | INFO | train | epoch 005 | loss 7.749 | nll_loss 6.476 | glat_accu 0 | glat_context_p 0.466 | word_ins 7.589 | length 4.451 | ppl 215.18 | wps 593.7 | ups 1.47 | wpb 403.7 | bsz 17.5 | num_updates 56390 | lr 0.000133168 | gnorm 0.779 | clip 0.1 | loss_scale 1024 | train_wall 7153 | gb_free 13.4 | wall 38359
2021-08-20 14:46:03 | INFO | fairseq.trainer | begin training epoch 6
2021-08-20 14:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 14:51:53 | INFO | train_inner | epoch 006:    610 / 11278 loss=7.731, nll_loss=6.455, glat_accu=0, glat_context_p=0.462, word_ins=7.57, length=4.447, ppl=212.46, wps=366.5, ups=0.92, wpb=396.8, bsz=17.4, num_updates=57000, lr=0.000132453, gnorm=0.744, clip=0, loss_scale=1024, train_wall=594, gb_free=13.6, wall=38710
2021-08-20 15:02:10 | INFO | train_inner | epoch 006:   1610 / 11278 loss=7.726, nll_loss=6.451, glat_accu=0, glat_context_p=0.462, word_ins=7.567, length=4.367, ppl=211.65, wps=673.6, ups=1.62, wpb=415.8, bsz=18.1, num_updates=58000, lr=0.000131306, gnorm=0.783, clip=0.6, loss_scale=1024, train_wall=615, gb_free=13.4, wall=39327
2021-08-20 15:12:33 | INFO | train_inner | epoch 006:   2610 / 11278 loss=7.747, nll_loss=6.473, glat_accu=0, glat_context_p=0.461, word_ins=7.586, length=4.411, ppl=214.78, wps=658, ups=1.61, wpb=409.8, bsz=17.7, num_updates=59000, lr=0.000130189, gnorm=0.764, clip=0.2, loss_scale=1024, train_wall=620, gb_free=13.7, wall=39950
2021-08-20 15:22:44 | INFO | train_inner | epoch 006:   3610 / 11278 loss=7.738, nll_loss=6.462, glat_accu=0, glat_context_p=0.46, word_ins=7.577, length=4.438, ppl=213.45, wps=643.8, ups=1.64, wpb=393.5, bsz=17.1, num_updates=60000, lr=0.000129099, gnorm=0.745, clip=0, loss_scale=1024, train_wall=609, gb_free=13.2, wall=40561
2021-08-20 15:33:03 | INFO | train_inner | epoch 006:   4610 / 11278 loss=7.764, nll_loss=6.489, glat_accu=0, glat_context_p=0.46, word_ins=7.6, length=4.5, ppl=217.33, wps=643.8, ups=1.62, wpb=398.6, bsz=17.2, num_updates=61000, lr=0.000128037, gnorm=0.726, clip=0, loss_scale=1024, train_wall=617, gb_free=13.4, wall=41180
2021-08-20 15:43:20 | INFO | train_inner | epoch 006:   5610 / 11278 loss=7.715, nll_loss=6.435, glat_accu=0, glat_context_p=0.459, word_ins=7.553, length=4.454, ppl=210.15, wps=652.8, ups=1.62, wpb=402.5, bsz=17.6, num_updates=62000, lr=0.000127, gnorm=0.751, clip=0, loss_scale=1024, train_wall=614, gb_free=12.8, wall=41797
2021-08-20 15:53:42 | INFO | train_inner | epoch 006:   6610 / 11278 loss=7.759, nll_loss=6.487, glat_accu=0, glat_context_p=0.458, word_ins=7.598, length=4.437, ppl=216.66, wps=630, ups=1.61, wpb=392.1, bsz=17, num_updates=63000, lr=0.000125988, gnorm=0.768, clip=0.4, loss_scale=1024, train_wall=620, gb_free=13.4, wall=42419
2021-08-20 16:04:00 | INFO | train_inner | epoch 006:   7610 / 11278 loss=7.756, nll_loss=6.479, glat_accu=0, glat_context_p=0.458, word_ins=7.592, length=4.503, ppl=216.14, wps=649.7, ups=1.62, wpb=401.5, bsz=17.4, num_updates=64000, lr=0.000125, gnorm=0.727, clip=0, loss_scale=1024, train_wall=615, gb_free=13, wall=43037
2021-08-20 16:14:20 | INFO | train_inner | epoch 006:   8610 / 11278 loss=7.732, nll_loss=6.452, glat_accu=0, glat_context_p=0.457, word_ins=7.568, length=4.503, ppl=212.59, wps=664.8, ups=1.61, wpb=412.3, bsz=18.1, num_updates=65000, lr=0.000124035, gnorm=0.705, clip=0, loss_scale=1024, train_wall=618, gb_free=14, wall=43657
2021-08-20 16:24:47 | INFO | train_inner | epoch 006:   9610 / 11278 loss=7.754, nll_loss=6.481, glat_accu=0, glat_context_p=0.456, word_ins=7.593, length=4.421, ppl=215.88, wps=653.1, ups=1.6, wpb=409.4, bsz=17.7, num_updates=66000, lr=0.000123091, gnorm=0.715, clip=0, loss_scale=2048, train_wall=624, gb_free=13.1, wall=44284
2021-08-20 16:35:11 | INFO | train_inner | epoch 006:  10610 / 11278 loss=7.747, nll_loss=6.472, glat_accu=0, glat_context_p=0.456, word_ins=7.586, length=4.441, ppl=214.83, wps=658, ups=1.6, wpb=410.6, bsz=17.8, num_updates=67000, lr=0.000122169, gnorm=0.749, clip=0.1, loss_scale=2048, train_wall=622, gb_free=13.5, wall=44908
2021-08-20 16:42:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 16:49:52 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.201 | nll_loss 6.887 | word_ins 7.982 | length 4.374 | ppl 294.31 | bleu 0 | wps 548.1 | wpb 184.6 | bsz 8 | num_updates 67668 | best_bleu 0
2021-08-20 16:49:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 67668 updates
2021-08-20 16:49:52 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint6.pt
2021-08-20 16:49:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint6.pt
2021-08-20 16:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint6.pt (epoch 6 @ 67668 updates, score 0.0) (writing took 18.176996321999468 seconds)
2021-08-20 16:50:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-08-20 16:50:10 | INFO | train | epoch 006 | loss 7.744 | nll_loss 6.469 | glat_accu 0 | glat_context_p 0.459 | word_ins 7.582 | length 4.45 | ppl 214.41 | wps 611.3 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 67668 | lr 0.000121565 | gnorm 0.742 | clip 0.1 | loss_scale 2048 | train_wall 6939 | gb_free 13.1 | wall 45807
2021-08-20 16:50:10 | INFO | fairseq.trainer | begin training epoch 7
2021-08-20 16:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 16:53:37 | INFO | train_inner | epoch 007:    332 / 11278 loss=7.751, nll_loss=6.476, glat_accu=0, glat_context_p=0.455, word_ins=7.589, length=4.472, ppl=215.49, wps=358, ups=0.9, wpb=396, bsz=17.2, num_updates=68000, lr=0.000121268, gnorm=0.725, clip=0, loss_scale=2048, train_wall=622, gb_free=14.4, wall=46014
2021-08-20 17:03:59 | INFO | train_inner | epoch 007:   1332 / 11278 loss=7.729, nll_loss=6.449, glat_accu=0, glat_context_p=0.454, word_ins=7.565, length=4.484, ppl=212.21, wps=636.7, ups=1.61, wpb=395.6, bsz=17.3, num_updates=69000, lr=0.000120386, gnorm=0.745, clip=0.2, loss_scale=2048, train_wall=619, gb_free=13.8, wall=46635
2021-08-20 17:14:19 | INFO | train_inner | epoch 007:   2332 / 11278 loss=7.728, nll_loss=6.448, glat_accu=0, glat_context_p=0.454, word_ins=7.564, length=4.479, ppl=212.04, wps=661.7, ups=1.61, wpb=410.3, bsz=17.9, num_updates=70000, lr=0.000119523, gnorm=0.714, clip=0, loss_scale=2048, train_wall=617, gb_free=13.6, wall=47255
2021-08-20 17:24:38 | INFO | train_inner | epoch 007:   3332 / 11278 loss=7.739, nll_loss=6.462, glat_accu=0, glat_context_p=0.453, word_ins=7.577, length=4.428, ppl=213.62, wps=648.2, ups=1.62, wpb=401.3, bsz=17.5, num_updates=71000, lr=0.000118678, gnorm=0.711, clip=0, loss_scale=2048, train_wall=617, gb_free=12.9, wall=47874
2021-08-20 17:35:03 | INFO | train_inner | epoch 007:   4332 / 11278 loss=7.755, nll_loss=6.48, glat_accu=0, glat_context_p=0.452, word_ins=7.593, length=4.456, ppl=216.08, wps=660.1, ups=1.6, wpb=412.7, bsz=17.8, num_updates=72000, lr=0.000117851, gnorm=0.761, clip=0.2, loss_scale=2048, train_wall=623, gb_free=13, wall=48500
2021-08-20 17:45:22 | INFO | train_inner | epoch 007:   5332 / 11278 loss=7.736, nll_loss=6.461, glat_accu=0, glat_context_p=0.452, word_ins=7.576, length=4.376, ppl=213.18, wps=654.4, ups=1.62, wpb=404.9, bsz=17.7, num_updates=73000, lr=0.000117041, gnorm=0.709, clip=0, loss_scale=2048, train_wall=616, gb_free=13.8, wall=49118
2021-08-20 17:55:17 | INFO | train_inner | epoch 007:   6332 / 11278 loss=7.742, nll_loss=6.465, glat_accu=0, glat_context_p=0.451, word_ins=7.579, length=4.448, ppl=214.06, wps=680, ups=1.68, wpb=404.8, bsz=17.5, num_updates=74000, lr=0.000116248, gnorm=0.742, clip=0.2, loss_scale=2048, train_wall=593, gb_free=13.6, wall=49714
2021-08-20 18:05:33 | INFO | train_inner | epoch 007:   7332 / 11278 loss=7.746, nll_loss=6.469, glat_accu=0, glat_context_p=0.45, word_ins=7.583, length=4.448, ppl=214.64, wps=655.3, ups=1.62, wpb=403.9, bsz=17.4, num_updates=75000, lr=0.00011547, gnorm=0.703, clip=0, loss_scale=2048, train_wall=614, gb_free=14, wall=50330
2021-08-20 18:15:57 | INFO | train_inner | epoch 007:   8332 / 11278 loss=7.743, nll_loss=6.464, glat_accu=0, glat_context_p=0.45, word_ins=7.578, length=4.486, ppl=214.16, wps=644.8, ups=1.6, wpb=402.1, bsz=17.4, num_updates=76000, lr=0.000114708, gnorm=0.753, clip=0.4, loss_scale=2048, train_wall=621, gb_free=13, wall=50954
2021-08-20 18:26:17 | INFO | train_inner | epoch 007:   9332 / 11278 loss=7.746, nll_loss=6.469, glat_accu=0, glat_context_p=0.449, word_ins=7.583, length=4.434, ppl=214.61, wps=637.2, ups=1.61, wpb=395.2, bsz=17.2, num_updates=77000, lr=0.000113961, gnorm=0.7, clip=0, loss_scale=2048, train_wall=618, gb_free=13.4, wall=51574
2021-08-20 18:36:38 | INFO | train_inner | epoch 007:  10332 / 11278 loss=7.745, nll_loss=6.467, glat_accu=0, glat_context_p=0.448, word_ins=7.581, length=4.463, ppl=214.54, wps=659.2, ups=1.61, wpb=409.5, bsz=17.8, num_updates=78000, lr=0.000113228, gnorm=0.694, clip=0, loss_scale=2048, train_wall=619, gb_free=13.7, wall=52195
2021-08-20 18:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 18:54:04 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.135 | nll_loss 6.815 | word_ins 7.917 | length 4.367 | ppl 281.19 | bleu 0 | wps 548.9 | wpb 184.6 | bsz 8 | num_updates 78946 | best_bleu 0
2021-08-20 18:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 78946 updates
2021-08-20 18:54:04 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint7.pt
2021-08-20 18:54:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint7.pt
2021-08-20 18:54:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint7.pt (epoch 7 @ 78946 updates, score 0.0) (writing took 19.36038325401023 seconds)
2021-08-20 18:54:23 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-08-20 18:54:23 | INFO | train | epoch 007 | loss 7.74 | nll_loss 6.463 | glat_accu 0 | glat_context_p 0.451 | word_ins 7.577 | length 4.449 | ppl 213.84 | wps 610.9 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 78946 | lr 0.000112547 | gnorm 0.724 | clip 0.1 | loss_scale 2048 | train_wall 6943 | gb_free 13.2 | wall 53260
2021-08-20 18:54:23 | INFO | fairseq.trainer | begin training epoch 8
2021-08-20 18:54:23 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 18:54:56 | INFO | train_inner | epoch 008:     54 / 11278 loss=7.735, nll_loss=6.457, glat_accu=0, glat_context_p=0.448, word_ins=7.572, length=4.437, ppl=212.97, wps=366.9, ups=0.91, wpb=402.7, bsz=17.5, num_updates=79000, lr=0.000112509, gnorm=0.723, clip=0.1, loss_scale=2048, train_wall=614, gb_free=13.8, wall=53293
2021-08-20 19:05:20 | INFO | train_inner | epoch 008:   1054 / 11278 loss=7.736, nll_loss=6.457, glat_accu=0, glat_context_p=0.447, word_ins=7.572, length=4.458, ppl=213.18, wps=659.4, ups=1.6, wpb=411.1, bsz=17.9, num_updates=80000, lr=0.000111803, gnorm=0.686, clip=0, loss_scale=2048, train_wall=621, gb_free=13.3, wall=53916
2021-08-20 19:15:39 | INFO | train_inner | epoch 008:   2054 / 11278 loss=7.744, nll_loss=6.467, glat_accu=0, glat_context_p=0.446, word_ins=7.581, length=4.444, ppl=214.45, wps=660.3, ups=1.61, wpb=409, bsz=17.6, num_updates=81000, lr=0.000111111, gnorm=0.694, clip=0, loss_scale=2048, train_wall=617, gb_free=13.6, wall=54536
2021-08-20 19:25:56 | INFO | train_inner | epoch 008:   3054 / 11278 loss=7.721, nll_loss=6.44, glat_accu=0, glat_context_p=0.446, word_ins=7.557, length=4.454, ppl=210.97, wps=642.7, ups=1.62, wpb=396.5, bsz=17.3, num_updates=82000, lr=0.000110432, gnorm=0.738, clip=0.1, loss_scale=4096, train_wall=615, gb_free=13.4, wall=55153
2021-08-20 19:36:21 | INFO | train_inner | epoch 008:   4054 / 11278 loss=7.743, nll_loss=6.465, glat_accu=0, glat_context_p=0.445, word_ins=7.58, length=4.46, ppl=214.28, wps=656.8, ups=1.6, wpb=410.3, bsz=17.7, num_updates=83000, lr=0.000109764, gnorm=0.693, clip=0, loss_scale=4096, train_wall=622, gb_free=13.6, wall=55777
2021-08-20 19:46:39 | INFO | train_inner | epoch 008:   5054 / 11278 loss=7.73, nll_loss=6.451, glat_accu=0, glat_context_p=0.444, word_ins=7.567, length=4.421, ppl=212.28, wps=665.2, ups=1.62, wpb=411.5, bsz=17.8, num_updates=84000, lr=0.000109109, gnorm=0.694, clip=0, loss_scale=4096, train_wall=616, gb_free=14.3, wall=56396
2021-08-20 19:57:01 | INFO | train_inner | epoch 008:   6054 / 11278 loss=7.747, nll_loss=6.465, glat_accu=0, glat_context_p=0.444, word_ins=7.58, length=4.525, ppl=214.83, wps=634.3, ups=1.61, wpb=394.6, bsz=17.1, num_updates=85000, lr=0.000108465, gnorm=0.75, clip=0.1, loss_scale=4096, train_wall=620, gb_free=13.5, wall=57018
2021-08-20 20:07:20 | INFO | train_inner | epoch 008:   7054 / 11278 loss=7.724, nll_loss=6.445, glat_accu=0, glat_context_p=0.443, word_ins=7.562, length=4.383, ppl=211.43, wps=656.4, ups=1.62, wpb=405.9, bsz=17.6, num_updates=86000, lr=0.000107833, gnorm=0.692, clip=0, loss_scale=4096, train_wall=616, gb_free=13.7, wall=57637
2021-08-20 20:17:42 | INFO | train_inner | epoch 008:   8054 / 11278 loss=7.745, nll_loss=6.469, glat_accu=0, glat_context_p=0.442, word_ins=7.583, length=4.397, ppl=214.5, wps=643.6, ups=1.61, wpb=400.7, bsz=17.4, num_updates=87000, lr=0.000107211, gnorm=0.693, clip=0, loss_scale=4096, train_wall=620, gb_free=13.6, wall=58259
2021-08-20 20:28:01 | INFO | train_inner | epoch 008:   9054 / 11278 loss=7.739, nll_loss=6.46, glat_accu=0, glat_context_p=0.442, word_ins=7.575, length=4.443, ppl=213.66, wps=648.7, ups=1.62, wpb=401.5, bsz=17.3, num_updates=88000, lr=0.0001066, gnorm=0.715, clip=0, loss_scale=4096, train_wall=617, gb_free=13.4, wall=58878
2021-08-20 20:38:19 | INFO | train_inner | epoch 008:  10054 / 11278 loss=7.721, nll_loss=6.44, glat_accu=0, glat_context_p=0.441, word_ins=7.557, length=4.44, ppl=211, wps=642.9, ups=1.62, wpb=396.8, bsz=17.4, num_updates=89000, lr=0.000106, gnorm=0.705, clip=0, loss_scale=4096, train_wall=615, gb_free=13.2, wall=59495
2021-08-20 20:48:18 | INFO | train_inner | epoch 008:  11054 / 11278 loss=7.73, nll_loss=6.448, glat_accu=0, glat_context_p=0.44, word_ins=7.564, length=4.493, ppl=212.36, wps=678.8, ups=1.67, wpb=406.8, bsz=17.8, num_updates=90000, lr=0.000105409, gnorm=0.682, clip=0, loss_scale=4096, train_wall=597, gb_free=12.8, wall=60095
2021-08-20 20:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 20:58:24 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.172 | nll_loss 6.884 | word_ins 7.954 | length 4.369 | ppl 288.48 | bleu 0 | wps 544.5 | wpb 184.6 | bsz 8 | num_updates 90224 | best_bleu 0
2021-08-20 20:58:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 90224 updates
2021-08-20 20:58:24 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint8.pt
2021-08-20 20:58:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint8.pt
2021-08-20 20:58:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint8.pt (epoch 8 @ 90224 updates, score 0.0) (writing took 19.242586817999836 seconds)
2021-08-20 20:58:43 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-08-20 20:58:43 | INFO | train | epoch 008 | loss 7.736 | nll_loss 6.456 | glat_accu 0 | glat_context_p 0.444 | word_ins 7.572 | length 4.449 | ppl 213.16 | wps 610.3 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 90224 | lr 0.000105278 | gnorm 0.703 | clip 0 | loss_scale 4096 | train_wall 6948 | gb_free 13.5 | wall 60720
2021-08-20 20:58:43 | INFO | fairseq.trainer | begin training epoch 9
2021-08-20 20:58:43 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 21:06:47 | INFO | train_inner | epoch 009:    776 / 11278 loss=7.754, nll_loss=6.474, glat_accu=0, glat_context_p=0.44, word_ins=7.587, length=4.525, ppl=215.86, wps=358.1, ups=0.9, wpb=397.1, bsz=17.1, num_updates=91000, lr=0.000104828, gnorm=0.694, clip=0, loss_scale=4096, train_wall=621, gb_free=13.3, wall=61203
2021-08-20 21:16:59 | INFO | train_inner | epoch 009:   1776 / 11278 loss=7.727, nll_loss=6.447, glat_accu=0, glat_context_p=0.439, word_ins=7.563, length=4.437, ppl=211.92, wps=636.8, ups=1.63, wpb=390.1, bsz=16.8, num_updates=92000, lr=0.000104257, gnorm=0.718, clip=0, loss_scale=4096, train_wall=610, gb_free=13.7, wall=61816
2021-08-20 21:27:16 | INFO | train_inner | epoch 009:   2776 / 11278 loss=7.709, nll_loss=6.424, glat_accu=0, glat_context_p=0.438, word_ins=7.543, length=4.466, ppl=209.17, wps=640.1, ups=1.62, wpb=395, bsz=17.2, num_updates=93000, lr=0.000103695, gnorm=0.728, clip=0.3, loss_scale=4096, train_wall=615, gb_free=13.3, wall=62433
2021-08-20 21:37:40 | INFO | train_inner | epoch 009:   3776 / 11278 loss=7.727, nll_loss=6.446, glat_accu=0, glat_context_p=0.438, word_ins=7.563, length=4.418, ppl=211.8, wps=654.9, ups=1.6, wpb=408.1, bsz=17.8, num_updates=94000, lr=0.000103142, gnorm=0.686, clip=0, loss_scale=4096, train_wall=621, gb_free=13.8, wall=63056
2021-08-20 21:48:00 | INFO | train_inner | epoch 009:   4776 / 11278 loss=7.738, nll_loss=6.459, glat_accu=0, glat_context_p=0.437, word_ins=7.575, length=4.402, ppl=213.51, wps=651.8, ups=1.61, wpb=404.5, bsz=17.6, num_updates=95000, lr=0.000102598, gnorm=0.688, clip=0, loss_scale=4096, train_wall=618, gb_free=13.3, wall=63677
2021-08-20 21:58:27 | INFO | train_inner | epoch 009:   5776 / 11278 loss=7.732, nll_loss=6.453, glat_accu=0, glat_context_p=0.436, word_ins=7.569, length=4.393, ppl=212.6, wps=640.4, ups=1.6, wpb=401.4, bsz=17.5, num_updates=96000, lr=0.000102062, gnorm=0.708, clip=0.2, loss_scale=4096, train_wall=625, gb_free=13.6, wall=64304
2021-08-20 22:08:48 | INFO | train_inner | epoch 009:   6776 / 11278 loss=7.727, nll_loss=6.446, glat_accu=0, glat_context_p=0.436, word_ins=7.563, length=4.419, ppl=211.86, wps=661.7, ups=1.61, wpb=410.8, bsz=18, num_updates=97000, lr=0.000101535, gnorm=0.68, clip=0, loss_scale=4096, train_wall=618, gb_free=13.7, wall=64925
2021-08-20 22:19:12 | INFO | train_inner | epoch 009:   7776 / 11278 loss=7.741, nll_loss=6.46, glat_accu=0, glat_context_p=0.435, word_ins=7.575, length=4.453, ppl=213.91, wps=665.5, ups=1.6, wpb=415.2, bsz=17.9, num_updates=98000, lr=0.000101015, gnorm=0.679, clip=0, loss_scale=4096, train_wall=621, gb_free=13.6, wall=65549
2021-08-20 22:29:34 | INFO | train_inner | epoch 009:   8776 / 11278 loss=7.758, nll_loss=6.481, glat_accu=0, glat_context_p=0.434, word_ins=7.594, length=4.424, ppl=216.43, wps=656.2, ups=1.61, wpb=408, bsz=17.7, num_updates=99000, lr=0.000100504, gnorm=0.707, clip=0, loss_scale=8192, train_wall=620, gb_free=13.8, wall=66171
2021-08-20 22:39:51 | INFO | train_inner | epoch 009:   9776 / 11278 loss=7.728, nll_loss=6.446, glat_accu=0, glat_context_p=0.434, word_ins=7.562, length=4.447, ppl=211.99, wps=642.4, ups=1.62, wpb=396.5, bsz=17.3, num_updates=100000, lr=0.0001, gnorm=0.705, clip=0, loss_scale=8192, train_wall=615, gb_free=13.4, wall=66788
2021-08-20 22:50:14 | INFO | train_inner | epoch 009:  10776 / 11278 loss=7.744, nll_loss=6.462, glat_accu=0, glat_context_p=0.433, word_ins=7.577, length=4.494, ppl=214.37, wps=658.3, ups=1.61, wpb=410, bsz=17.8, num_updates=101000, lr=9.95037e-05, gnorm=0.688, clip=0, loss_scale=8192, train_wall=620, gb_free=13.1, wall=67411
2021-08-20 22:55:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-20 23:03:12 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.216 | nll_loss 6.941 | word_ins 7.997 | length 4.374 | ppl 297.36 | bleu 0 | wps 544.9 | wpb 184.6 | bsz 8 | num_updates 101502 | best_bleu 0
2021-08-20 23:03:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 101502 updates
2021-08-20 23:03:12 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint9.pt
2021-08-20 23:03:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint9.pt
2021-08-20 23:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint9.pt (epoch 9 @ 101502 updates, score 0.0) (writing took 27.856871520983987 seconds)
2021-08-20 23:03:40 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-08-20 23:03:40 | INFO | train | epoch 009 | loss 7.735 | nll_loss 6.454 | glat_accu 0 | glat_context_p 0.436 | word_ins 7.57 | length 4.448 | ppl 213.08 | wps 607.4 | ups 1.5 | wpb 403.7 | bsz 17.5 | num_updates 101502 | lr 9.92574e-05 | gnorm 0.697 | clip 0 | loss_scale 8192 | train_wall 6976 | gb_free 13.4 | wall 68216
2021-08-20 23:03:40 | INFO | fairseq.trainer | begin training epoch 10
2021-08-20 23:03:40 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-20 23:08:50 | INFO | train_inner | epoch 010:    498 / 11278 loss=7.734, nll_loss=6.452, glat_accu=0, glat_context_p=0.432, word_ins=7.568, length=4.486, ppl=212.97, wps=375, ups=0.9, wpb=418.4, bsz=18.1, num_updates=102000, lr=9.90148e-05, gnorm=0.673, clip=0, loss_scale=8192, train_wall=620, gb_free=13.5, wall=68526
2021-08-20 23:19:16 | INFO | train_inner | epoch 010:   1498 / 11278 loss=7.754, nll_loss=6.474, glat_accu=0, glat_context_p=0.432, word_ins=7.588, length=4.48, ppl=215.84, wps=647.9, ups=1.6, wpb=405.5, bsz=17.6, num_updates=103000, lr=9.85329e-05, gnorm=0.68, clip=0, loss_scale=8192, train_wall=623, gb_free=13.7, wall=69152
2021-08-20 23:29:36 | INFO | train_inner | epoch 010:   2498 / 11278 loss=7.726, nll_loss=6.441, glat_accu=0, glat_context_p=0.431, word_ins=7.559, length=4.506, ppl=211.75, wps=661.7, ups=1.61, wpb=410.4, bsz=17.8, num_updates=104000, lr=9.80581e-05, gnorm=0.701, clip=0.1, loss_scale=8192, train_wall=618, gb_free=13.1, wall=69772
2021-08-20 23:39:34 | INFO | train_inner | epoch 010:   3498 / 11278 loss=7.739, nll_loss=6.457, glat_accu=0, glat_context_p=0.43, word_ins=7.572, length=4.484, ppl=213.65, wps=657.6, ups=1.67, wpb=393.3, bsz=17, num_updates=105000, lr=9.759e-05, gnorm=0.693, clip=0, loss_scale=8192, train_wall=596, gb_free=13.1, wall=70371
2021-08-20 23:49:55 | INFO | train_inner | epoch 010:   4498 / 11278 loss=7.713, nll_loss=6.429, glat_accu=0, glat_context_p=0.43, word_ins=7.548, length=4.44, ppl=209.83, wps=650.3, ups=1.61, wpb=403.7, bsz=17.7, num_updates=106000, lr=9.71286e-05, gnorm=0.687, clip=0, loss_scale=8192, train_wall=618, gb_free=14.3, wall=70991
2021-08-21 00:00:10 | INFO | train_inner | epoch 010:   5498 / 11278 loss=7.743, nll_loss=6.463, glat_accu=0, glat_context_p=0.429, word_ins=7.578, length=4.451, ppl=214.3, wps=652.8, ups=1.62, wpb=401.8, bsz=17.3, num_updates=107000, lr=9.66736e-05, gnorm=0.716, clip=0.2, loss_scale=8192, train_wall=613, gb_free=12.7, wall=71607
2021-08-21 00:10:29 | INFO | train_inner | epoch 010:   6498 / 11278 loss=7.724, nll_loss=6.438, glat_accu=0, glat_context_p=0.428, word_ins=7.556, length=4.471, ppl=211.37, wps=647.2, ups=1.62, wpb=400.1, bsz=17.5, num_updates=108000, lr=9.6225e-05, gnorm=0.685, clip=0, loss_scale=8192, train_wall=616, gb_free=12.7, wall=72225
2021-08-21 00:20:55 | INFO | train_inner | epoch 010:   7498 / 11278 loss=7.758, nll_loss=6.479, glat_accu=0, glat_context_p=0.428, word_ins=7.592, length=4.439, ppl=216.53, wps=635.3, ups=1.6, wpb=398, bsz=17.1, num_updates=109000, lr=9.57826e-05, gnorm=0.705, clip=0, loss_scale=8192, train_wall=624, gb_free=12.8, wall=72852
2021-08-21 00:31:11 | INFO | train_inner | epoch 010:   8498 / 11278 loss=7.701, nll_loss=6.414, glat_accu=0, glat_context_p=0.427, word_ins=7.534, length=4.448, ppl=208.11, wps=645.6, ups=1.62, wpb=397.9, bsz=17.5, num_updates=110000, lr=9.53463e-05, gnorm=0.688, clip=0, loss_scale=8192, train_wall=614, gb_free=12.8, wall=73468
2021-08-21 00:41:37 | INFO | train_inner | epoch 010:   9498 / 11278 loss=7.751, nll_loss=6.472, glat_accu=0, glat_context_p=0.426, word_ins=7.586, length=4.427, ppl=215.46, wps=645, ups=1.6, wpb=403.5, bsz=17.6, num_updates=111000, lr=9.49158e-05, gnorm=0.721, clip=0.4, loss_scale=8192, train_wall=623, gb_free=13.1, wall=74094
2021-08-21 00:52:01 | INFO | train_inner | epoch 010:  10498 / 11278 loss=7.739, nll_loss=6.459, glat_accu=0, glat_context_p=0.426, word_ins=7.574, length=4.405, ppl=213.66, wps=658.1, ups=1.6, wpb=410.7, bsz=17.8, num_updates=112000, lr=9.44911e-05, gnorm=0.669, clip=0, loss_scale=8192, train_wall=622, gb_free=12.9, wall=74718
2021-08-21 01:00:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 01:07:46 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.228 | nll_loss 6.921 | word_ins 8.009 | length 4.37 | ppl 299.77 | bleu 0 | wps 551.4 | wpb 184.6 | bsz 8 | num_updates 112780 | best_bleu 0
2021-08-21 01:07:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 112780 updates
2021-08-21 01:07:46 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint10.pt
2021-08-21 01:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint10.pt
2021-08-21 01:08:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint10.pt (epoch 10 @ 112780 updates, score 0.0) (writing took 23.36380116699729 seconds)
2021-08-21 01:08:10 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-08-21 01:08:10 | INFO | train | epoch 010 | loss 7.734 | nll_loss 6.451 | glat_accu 0 | glat_context_p 0.429 | word_ins 7.568 | length 4.448 | ppl 212.84 | wps 609.5 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 112780 | lr 9.41638e-05 | gnorm 0.695 | clip 0.1 | loss_scale 8192 | train_wall 6959 | gb_free 13.8 | wall 75686
2021-08-21 01:08:10 | INFO | fairseq.trainer | begin training epoch 11
2021-08-21 01:08:10 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 01:10:26 | INFO | train_inner | epoch 011:    220 / 11278 loss=7.737, nll_loss=6.455, glat_accu=0, glat_context_p=0.425, word_ins=7.571, length=4.423, ppl=213.27, wps=367.2, ups=0.91, wpb=405.6, bsz=17.5, num_updates=113000, lr=9.40721e-05, gnorm=0.706, clip=0.2, loss_scale=8192, train_wall=619, gb_free=12.9, wall=75822
2021-08-21 01:20:50 | INFO | train_inner | epoch 011:   1220 / 11278 loss=7.746, nll_loss=6.464, glat_accu=0, glat_context_p=0.424, word_ins=7.579, length=4.455, ppl=214.69, wps=654.4, ups=1.6, wpb=408.7, bsz=17.7, num_updates=114000, lr=9.36586e-05, gnorm=0.673, clip=0, loss_scale=8192, train_wall=622, gb_free=13.8, wall=76447
2021-08-21 01:31:08 | INFO | train_inner | epoch 011:   2220 / 11278 loss=7.723, nll_loss=6.438, glat_accu=0, glat_context_p=0.424, word_ins=7.555, length=4.464, ppl=211.27, wps=654.2, ups=1.62, wpb=404.5, bsz=17.6, num_updates=115000, lr=9.32505e-05, gnorm=0.682, clip=0, loss_scale=16384, train_wall=616, gb_free=13.6, wall=77065
2021-08-21 01:41:28 | INFO | train_inner | epoch 011:   3220 / 11278 loss=7.719, nll_loss=6.434, glat_accu=0, glat_context_p=0.423, word_ins=7.553, length=4.423, ppl=210.63, wps=625.7, ups=1.61, wpb=387.6, bsz=17, num_updates=116000, lr=9.28477e-05, gnorm=0.695, clip=0, loss_scale=16384, train_wall=617, gb_free=12.2, wall=77685
2021-08-21 01:51:46 | INFO | train_inner | epoch 011:   4220 / 11278 loss=7.727, nll_loss=6.443, glat_accu=0, glat_context_p=0.422, word_ins=7.56, length=4.448, ppl=211.84, wps=640.5, ups=1.62, wpb=396, bsz=17.3, num_updates=117000, lr=9.245e-05, gnorm=0.702, clip=0, loss_scale=16384, train_wall=616, gb_free=13.7, wall=78303
2021-08-21 02:02:10 | INFO | train_inner | epoch 011:   5220 / 11278 loss=7.73, nll_loss=6.445, glat_accu=0, glat_context_p=0.422, word_ins=7.562, length=4.483, ppl=212.29, wps=664.4, ups=1.6, wpb=414.7, bsz=18, num_updates=118000, lr=9.20575e-05, gnorm=0.697, clip=0.1, loss_scale=16384, train_wall=622, gb_free=13.3, wall=78927
2021-08-21 02:12:33 | INFO | train_inner | epoch 011:   6220 / 11278 loss=7.716, nll_loss=6.432, glat_accu=0, glat_context_p=0.421, word_ins=7.55, length=4.414, ppl=210.32, wps=659.7, ups=1.61, wpb=410.9, bsz=18, num_updates=119000, lr=9.16698e-05, gnorm=0.687, clip=0, loss_scale=16384, train_wall=620, gb_free=13.5, wall=79550
2021-08-21 02:22:59 | INFO | train_inner | epoch 011:   7220 / 11278 loss=7.732, nll_loss=6.448, glat_accu=0, glat_context_p=0.42, word_ins=7.565, length=4.441, ppl=212.54, wps=636.2, ups=1.6, wpb=398.4, bsz=17.4, num_updates=120000, lr=9.12871e-05, gnorm=0.672, clip=0, loss_scale=16384, train_wall=624, gb_free=13, wall=80176
2021-08-21 02:33:21 | INFO | train_inner | epoch 011:   8220 / 11278 loss=7.735, nll_loss=6.453, glat_accu=0, glat_context_p=0.42, word_ins=7.569, length=4.417, ppl=213.1, wps=650.9, ups=1.61, wpb=404.5, bsz=17.6, num_updates=121000, lr=9.09091e-05, gnorm=0.687, clip=0, loss_scale=16384, train_wall=619, gb_free=13.5, wall=80798
2021-08-21 02:43:24 | INFO | train_inner | epoch 011:   9220 / 11278 loss=7.778, nll_loss=6.499, glat_accu=0, glat_context_p=0.419, word_ins=7.61, length=4.48, ppl=219.49, wps=672, ups=1.66, wpb=405.3, bsz=17.2, num_updates=122000, lr=9.05357e-05, gnorm=0.717, clip=0.2, loss_scale=16384, train_wall=601, gb_free=13.3, wall=81401
2021-08-21 02:53:42 | INFO | train_inner | epoch 011:  10220 / 11278 loss=7.733, nll_loss=6.449, glat_accu=0, glat_context_p=0.418, word_ins=7.565, length=4.449, ppl=212.72, wps=645.2, ups=1.62, wpb=398.4, bsz=17.3, num_updates=123000, lr=9.0167e-05, gnorm=0.668, clip=0, loss_scale=16384, train_wall=615, gb_free=13.8, wall=82018
2021-08-21 03:04:03 | INFO | train_inner | epoch 011:  11220 / 11278 loss=7.733, nll_loss=6.449, glat_accu=0, glat_context_p=0.418, word_ins=7.566, length=4.439, ppl=212.68, wps=660.8, ups=1.61, wpb=410.5, bsz=17.8, num_updates=124000, lr=8.98027e-05, gnorm=0.68, clip=0, loss_scale=16384, train_wall=619, gb_free=13.2, wall=82639
2021-08-21 03:04:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 03:12:20 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.25 | nll_loss 7.009 | word_ins 8.031 | length 4.372 | ppl 304.4 | bleu 0 | wps 548.9 | wpb 184.6 | bsz 8 | num_updates 124058 | best_bleu 0
2021-08-21 03:12:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 124058 updates
2021-08-21 03:12:20 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint11.pt
2021-08-21 03:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint11.pt
2021-08-21 03:12:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint11.pt (epoch 11 @ 124058 updates, score 0.0) (writing took 22.97955887502758 seconds)
2021-08-21 03:12:43 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-08-21 03:12:43 | INFO | train | epoch 011 | loss 7.734 | nll_loss 6.45 | glat_accu 0 | glat_context_p 0.421 | word_ins 7.567 | length 4.447 | ppl 212.85 | wps 609.2 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 124058 | lr 8.97817e-05 | gnorm 0.687 | clip 0 | loss_scale 16384 | train_wall 6961 | gb_free 12.9 | wall 83160
2021-08-21 03:12:43 | INFO | fairseq.trainer | begin training epoch 12
2021-08-21 03:12:43 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 03:22:09 | INFO | train_inner | epoch 012:    942 / 11278 loss=7.713, nll_loss=6.428, glat_accu=0, glat_context_p=0.417, word_ins=7.547, length=4.408, ppl=209.8, wps=360.8, ups=0.92, wpb=392, bsz=17.1, num_updates=125000, lr=8.94427e-05, gnorm=0.708, clip=0, loss_scale=16384, train_wall=599, gb_free=14.1, wall=83726
2021-08-21 03:32:35 | INFO | train_inner | epoch 012:   1942 / 11278 loss=7.735, nll_loss=6.45, glat_accu=0, glat_context_p=0.416, word_ins=7.567, length=4.449, ppl=213.06, wps=656.8, ups=1.6, wpb=411.3, bsz=17.7, num_updates=126000, lr=8.90871e-05, gnorm=0.682, clip=0, loss_scale=16384, train_wall=624, gb_free=13.4, wall=84352
2021-08-21 03:43:02 | INFO | train_inner | epoch 012:   2942 / 11278 loss=7.718, nll_loss=6.431, glat_accu=0, glat_context_p=0.416, word_ins=7.549, length=4.452, ppl=210.52, wps=646.7, ups=1.6, wpb=405.4, bsz=17.6, num_updates=127000, lr=8.87357e-05, gnorm=0.684, clip=0, loss_scale=16384, train_wall=624, gb_free=13.5, wall=84979
2021-08-21 03:53:32 | INFO | train_inner | epoch 012:   3942 / 11278 loss=7.756, nll_loss=6.473, glat_accu=0, glat_context_p=0.415, word_ins=7.587, length=4.481, ppl=216.19, wps=637.6, ups=1.59, wpb=401.3, bsz=17.2, num_updates=128000, lr=8.83883e-05, gnorm=0.688, clip=0, loss_scale=16384, train_wall=627, gb_free=12.8, wall=85608
2021-08-21 04:03:53 | INFO | train_inner | epoch 012:   4942 / 11278 loss=7.724, nll_loss=6.436, glat_accu=0, glat_context_p=0.414, word_ins=7.554, length=4.492, ppl=211.45, wps=636.5, ups=1.61, wpb=395.3, bsz=17.3, num_updates=129000, lr=8.80451e-05, gnorm=0.709, clip=0.2, loss_scale=16384, train_wall=619, gb_free=13.2, wall=86230
2021-08-21 04:14:13 | INFO | train_inner | epoch 012:   5942 / 11278 loss=7.731, nll_loss=6.447, glat_accu=0, glat_context_p=0.414, word_ins=7.564, length=4.408, ppl=212.4, wps=646.2, ups=1.61, wpb=401, bsz=17.4, num_updates=130000, lr=8.77058e-05, gnorm=0.678, clip=0, loss_scale=16384, train_wall=618, gb_free=13, wall=86850
2021-08-21 04:24:32 | INFO | train_inner | epoch 012:   6942 / 11278 loss=7.728, nll_loss=6.443, glat_accu=0, glat_context_p=0.413, word_ins=7.56, length=4.438, ppl=212.06, wps=652, ups=1.62, wpb=403, bsz=17.6, num_updates=131000, lr=8.73704e-05, gnorm=0.678, clip=0, loss_scale=16384, train_wall=616, gb_free=13.2, wall=87468
2021-08-21 04:34:49 | INFO | train_inner | epoch 012:   7942 / 11278 loss=7.726, nll_loss=6.44, glat_accu=0, glat_context_p=0.412, word_ins=7.558, length=4.435, ppl=211.67, wps=652.9, ups=1.62, wpb=403.3, bsz=17.5, num_updates=132000, lr=8.70388e-05, gnorm=0.694, clip=0.1, loss_scale=32768, train_wall=615, gb_free=13.6, wall=88086
2021-08-21 04:45:10 | INFO | train_inner | epoch 012:   8942 / 11278 loss=7.722, nll_loss=6.436, glat_accu=0, glat_context_p=0.412, word_ins=7.554, length=4.43, ppl=211.16, wps=642.9, ups=1.61, wpb=399.1, bsz=17.5, num_updates=133000, lr=8.6711e-05, gnorm=0.68, clip=0, loss_scale=32768, train_wall=618, gb_free=14.1, wall=88707
2021-08-21 04:55:31 | INFO | train_inner | epoch 012:   9942 / 11278 loss=7.74, nll_loss=6.455, glat_accu=0, glat_context_p=0.411, word_ins=7.571, length=4.464, ppl=213.8, wps=661.3, ups=1.61, wpb=410.6, bsz=17.9, num_updates=134000, lr=8.63868e-05, gnorm=0.671, clip=0, loss_scale=32768, train_wall=619, gb_free=13.3, wall=89328
2021-08-21 05:05:52 | INFO | train_inner | epoch 012:  10942 / 11278 loss=7.733, nll_loss=6.446, glat_accu=0, glat_context_p=0.41, word_ins=7.563, length=4.475, ppl=212.7, wps=658.2, ups=1.61, wpb=408.4, bsz=17.6, num_updates=135000, lr=8.60663e-05, gnorm=0.703, clip=0.2, loss_scale=32768, train_wall=618, gb_free=13, wall=89948
2021-08-21 05:09:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 05:17:06 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.234 | nll_loss 6.95 | word_ins 8.016 | length 4.37 | ppl 301.11 | bleu 0 | wps 545.7 | wpb 184.6 | bsz 8 | num_updates 135336 | best_bleu 0
2021-08-21 05:17:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 135336 updates
2021-08-21 05:17:06 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint12.pt
2021-08-21 05:17:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint12.pt
2021-08-21 05:17:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint12.pt (epoch 12 @ 135336 updates, score 0.0) (writing took 18.00217968499055 seconds)
2021-08-21 05:17:24 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-08-21 05:17:24 | INFO | train | epoch 012 | loss 7.73 | nll_loss 6.445 | glat_accu 0 | glat_context_p 0.414 | word_ins 7.562 | length 4.447 | ppl 212.36 | wps 608.6 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 135336 | lr 8.59594e-05 | gnorm 0.688 | clip 0 | loss_scale 32768 | train_wall 6971 | gb_free 13 | wall 90641
2021-08-21 05:17:24 | INFO | fairseq.trainer | begin training epoch 13
2021-08-21 05:17:24 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 05:24:18 | INFO | train_inner | epoch 013:    664 / 11278 loss=7.734, nll_loss=6.45, glat_accu=0, glat_context_p=0.41, word_ins=7.566, length=4.423, ppl=212.88, wps=375, ups=0.9, wpb=414.8, bsz=18, num_updates=136000, lr=8.57493e-05, gnorm=0.675, clip=0, loss_scale=32768, train_wall=621, gb_free=13.4, wall=91054
2021-08-21 05:33:52 | INFO | train_inner | epoch 013:   1664 / 11278 loss=7.722, nll_loss=6.433, glat_accu=0, glat_context_p=0.409, word_ins=7.552, length=4.475, ppl=211.13, wps=726, ups=1.74, wpb=416.8, bsz=18, num_updates=137000, lr=8.54358e-05, gnorm=0.69, clip=0, loss_scale=32768, train_wall=572, gb_free=13.4, wall=91629
2021-08-21 05:44:13 | INFO | train_inner | epoch 013:   2664 / 11278 loss=7.762, nll_loss=6.48, glat_accu=0, glat_context_p=0.408, word_ins=7.593, length=4.439, ppl=217, wps=646.5, ups=1.61, wpb=401.7, bsz=17.3, num_updates=138000, lr=8.51257e-05, gnorm=0.677, clip=0, loss_scale=32768, train_wall=619, gb_free=13.1, wall=92250
2021-08-21 05:54:42 | INFO | train_inner | epoch 013:   3664 / 11278 loss=7.724, nll_loss=6.436, glat_accu=0, glat_context_p=0.408, word_ins=7.554, length=4.447, ppl=211.37, wps=634.4, ups=1.59, wpb=399, bsz=17.3, num_updates=139000, lr=8.48189e-05, gnorm=0.688, clip=0, loss_scale=32768, train_wall=627, gb_free=13.4, wall=92879
2021-08-21 06:05:01 | INFO | train_inner | epoch 013:   4664 / 11278 loss=7.707, nll_loss=6.419, glat_accu=0, glat_context_p=0.407, word_ins=7.539, length=4.421, ppl=208.97, wps=666.8, ups=1.62, wpb=412.7, bsz=18.1, num_updates=140000, lr=8.45154e-05, gnorm=0.713, clip=0.3, loss_scale=32768, train_wall=616, gb_free=13.3, wall=93498
2021-08-21 06:15:22 | INFO | train_inner | epoch 013:   5664 / 11278 loss=7.724, nll_loss=6.439, glat_accu=0, glat_context_p=0.406, word_ins=7.557, length=4.39, ppl=211.48, wps=653.9, ups=1.61, wpb=406.1, bsz=17.7, num_updates=141000, lr=8.42152e-05, gnorm=0.683, clip=0, loss_scale=32768, train_wall=618, gb_free=13.5, wall=94119
2021-08-21 06:25:46 | INFO | train_inner | epoch 013:   6664 / 11278 loss=7.747, nll_loss=6.464, glat_accu=0, glat_context_p=0.406, word_ins=7.579, length=4.412, ppl=214.83, wps=644.2, ups=1.6, wpb=402, bsz=17.2, num_updates=142000, lr=8.39181e-05, gnorm=0.681, clip=0, loss_scale=32768, train_wall=622, gb_free=12.8, wall=94743
2021-08-21 06:36:07 | INFO | train_inner | epoch 013:   7664 / 11278 loss=7.745, nll_loss=6.46, glat_accu=0, glat_context_p=0.405, word_ins=7.575, length=4.464, ppl=214.49, wps=652.4, ups=1.61, wpb=404.8, bsz=17.6, num_updates=143000, lr=8.36242e-05, gnorm=0.682, clip=0, loss_scale=32768, train_wall=618, gb_free=13, wall=95364
2021-08-21 06:46:28 | INFO | train_inner | epoch 013:   8664 / 11278 loss=7.736, nll_loss=6.447, glat_accu=0, glat_context_p=0.404, word_ins=7.564, length=4.524, ppl=213.26, wps=638.4, ups=1.61, wpb=396.3, bsz=17.2, num_updates=144000, lr=8.33333e-05, gnorm=0.681, clip=0, loss_scale=32768, train_wall=618, gb_free=12.3, wall=95984
2021-08-21 06:56:49 | INFO | train_inner | epoch 013:   9664 / 11278 loss=7.736, nll_loss=6.45, glat_accu=0, glat_context_p=0.404, word_ins=7.567, length=4.463, ppl=213.24, wps=646.1, ups=1.61, wpb=401.4, bsz=17.5, num_updates=145000, lr=8.30455e-05, gnorm=0.718, clip=0.2, loss_scale=32768, train_wall=619, gb_free=13.5, wall=96605
2021-08-21 07:07:08 | INFO | train_inner | epoch 013:  10664 / 11278 loss=7.751, nll_loss=6.467, glat_accu=0, glat_context_p=0.403, word_ins=7.582, length=4.463, ppl=215.48, wps=634.2, ups=1.61, wpb=392.9, bsz=17, num_updates=146000, lr=8.27606e-05, gnorm=0.69, clip=0, loss_scale=32768, train_wall=617, gb_free=12.4, wall=97225
2021-08-21 07:13:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 07:21:17 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.195 | nll_loss 6.928 | word_ins 7.977 | length 4.368 | ppl 293.06 | bleu 0 | wps 546.5 | wpb 184.6 | bsz 8 | num_updates 146614 | best_bleu 0
2021-08-21 07:21:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 146614 updates
2021-08-21 07:21:17 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint13.pt
2021-08-21 07:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint13.pt
2021-08-21 07:21:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint13.pt (epoch 13 @ 146614 updates, score 0.0) (writing took 18.029139871010557 seconds)
2021-08-21 07:21:35 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-08-21 07:21:35 | INFO | train | epoch 013 | loss 7.735 | nll_loss 6.449 | glat_accu 0 | glat_context_p 0.406 | word_ins 7.566 | length 4.447 | ppl 213.09 | wps 611.1 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 146614 | lr 8.25871e-05 | gnorm 0.69 | clip 0 | loss_scale 32768 | train_wall 6941 | gb_free 14.3 | wall 98092
2021-08-21 07:21:35 | INFO | fairseq.trainer | begin training epoch 14
2021-08-21 07:21:35 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 07:25:38 | INFO | train_inner | epoch 014:    386 / 11278 loss=7.731, nll_loss=6.446, glat_accu=0, glat_context_p=0.402, word_ins=7.563, length=4.412, ppl=212.49, wps=367.2, ups=0.9, wpb=407.5, bsz=17.8, num_updates=147000, lr=8.24786e-05, gnorm=0.683, clip=0, loss_scale=32768, train_wall=625, gb_free=13.6, wall=98335
2021-08-21 07:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-21 07:36:02 | INFO | train_inner | epoch 014:   1387 / 11278 loss=7.706, nll_loss=6.415, glat_accu=0, glat_context_p=0.402, word_ins=7.536, length=4.457, ppl=208.74, wps=654.2, ups=1.6, wpb=408, bsz=17.9, num_updates=148000, lr=8.21995e-05, gnorm=0.68, clip=0, loss_scale=32768, train_wall=621, gb_free=13.7, wall=98958
2021-08-21 07:46:23 | INFO | train_inner | epoch 014:   2387 / 11278 loss=7.735, nll_loss=6.449, glat_accu=0, glat_context_p=0.401, word_ins=7.566, length=4.431, ppl=212.97, wps=663.3, ups=1.61, wpb=412.1, bsz=17.9, num_updates=149000, lr=8.19232e-05, gnorm=0.714, clip=0.2, loss_scale=32768, train_wall=619, gb_free=13.6, wall=99580
2021-08-21 07:56:40 | INFO | train_inner | epoch 014:   3387 / 11278 loss=7.723, nll_loss=6.434, glat_accu=0, glat_context_p=0.4, word_ins=7.553, length=4.453, ppl=211.21, wps=651.2, ups=1.62, wpb=402.1, bsz=17.5, num_updates=150000, lr=8.16497e-05, gnorm=0.698, clip=0, loss_scale=32768, train_wall=615, gb_free=12.9, wall=100197
2021-08-21 08:07:02 | INFO | train_inner | epoch 014:   4387 / 11278 loss=7.744, nll_loss=6.454, glat_accu=0, glat_context_p=0.4, word_ins=7.57, length=4.538, ppl=214.45, wps=661.3, ups=1.61, wpb=411.2, bsz=17.7, num_updates=151000, lr=8.13788e-05, gnorm=0.688, clip=0, loss_scale=32768, train_wall=619, gb_free=13.9, wall=100819
2021-08-21 08:17:25 | INFO | train_inner | epoch 014:   5387 / 11278 loss=7.721, nll_loss=6.43, glat_accu=0, glat_context_p=0.399, word_ins=7.549, length=4.466, ppl=211.06, wps=628.3, ups=1.61, wpb=391.2, bsz=17, num_updates=152000, lr=8.11107e-05, gnorm=0.701, clip=0, loss_scale=32768, train_wall=620, gb_free=13.5, wall=101442
2021-08-21 08:27:25 | INFO | train_inner | epoch 014:   6387 / 11278 loss=7.744, nll_loss=6.456, glat_accu=0, glat_context_p=0.398, word_ins=7.572, length=4.469, ppl=214.36, wps=665.4, ups=1.67, wpb=399.2, bsz=17.3, num_updates=153000, lr=8.08452e-05, gnorm=0.685, clip=0, loss_scale=32768, train_wall=598, gb_free=13.1, wall=102042
2021-08-21 08:37:44 | INFO | train_inner | epoch 014:   7387 / 11278 loss=7.749, nll_loss=6.463, glat_accu=0, glat_context_p=0.398, word_ins=7.579, length=4.423, ppl=215.07, wps=638.2, ups=1.61, wpb=395.3, bsz=17, num_updates=154000, lr=8.05823e-05, gnorm=0.696, clip=0, loss_scale=32768, train_wall=617, gb_free=13.4, wall=102661
2021-08-21 08:48:04 | INFO | train_inner | epoch 014:   8387 / 11278 loss=7.721, nll_loss=6.431, glat_accu=0, glat_context_p=0.397, word_ins=7.55, length=4.424, ppl=210.92, wps=661.1, ups=1.61, wpb=409.7, bsz=17.9, num_updates=155000, lr=8.03219e-05, gnorm=0.682, clip=0, loss_scale=32768, train_wall=617, gb_free=13.5, wall=103281
2021-08-21 08:58:27 | INFO | train_inner | epoch 014:   9387 / 11278 loss=7.743, nll_loss=6.455, glat_accu=0, glat_context_p=0.396, word_ins=7.571, length=4.483, ppl=214.28, wps=626.7, ups=1.6, wpb=390.6, bsz=16.9, num_updates=156000, lr=8.00641e-05, gnorm=0.694, clip=0, loss_scale=32768, train_wall=621, gb_free=13.5, wall=103904
2021-08-21 09:08:44 | INFO | train_inner | epoch 014:  10387 / 11278 loss=7.73, nll_loss=6.445, glat_accu=0, glat_context_p=0.396, word_ins=7.563, length=4.35, ppl=212.32, wps=654.6, ups=1.62, wpb=403.7, bsz=17.7, num_updates=157000, lr=7.98087e-05, gnorm=0.68, clip=0, loss_scale=32768, train_wall=614, gb_free=13.6, wall=104521
2021-08-21 09:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 09:25:47 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.245 | nll_loss 6.993 | word_ins 8.027 | length 4.369 | ppl 303.44 | bleu 0 | wps 543.2 | wpb 184.6 | bsz 8 | num_updates 157891 | best_bleu 0
2021-08-21 09:25:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 157891 updates
2021-08-21 09:25:47 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint14.pt
2021-08-21 09:25:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint14.pt
2021-08-21 09:26:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint14.pt (epoch 14 @ 157891 updates, score 0.0) (writing took 17.879360370978247 seconds)
2021-08-21 09:26:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-08-21 09:26:05 | INFO | train | epoch 014 | loss 7.732 | nll_loss 6.444 | glat_accu 0 | glat_context_p 0.398 | word_ins 7.562 | length 4.447 | ppl 212.65 | wps 609.4 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 157891 | lr 7.95832e-05 | gnorm 0.694 | clip 0.1 | loss_scale 32768 | train_wall 6958 | gb_free 13 | wall 105562
2021-08-21 09:26:05 | INFO | fairseq.trainer | begin training epoch 15
2021-08-21 09:26:05 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 09:27:12 | INFO | train_inner | epoch 015:    109 / 11278 loss=7.749, nll_loss=6.464, glat_accu=0, glat_context_p=0.395, word_ins=7.579, length=4.425, ppl=215.09, wps=376, ups=0.9, wpb=416.7, bsz=18, num_updates=158000, lr=7.95557e-05, gnorm=0.717, clip=0.4, loss_scale=32768, train_wall=621, gb_free=13.2, wall=105629
2021-08-21 09:37:31 | INFO | train_inner | epoch 015:   1109 / 11278 loss=7.721, nll_loss=6.431, glat_accu=0, glat_context_p=0.394, word_ins=7.55, length=4.424, ppl=210.94, wps=647.5, ups=1.62, wpb=400.4, bsz=17.4, num_updates=159000, lr=7.93052e-05, gnorm=0.689, clip=0, loss_scale=32768, train_wall=616, gb_free=14.1, wall=106248
2021-08-21 09:47:49 | INFO | train_inner | epoch 015:   2109 / 11278 loss=7.736, nll_loss=6.447, glat_accu=0, glat_context_p=0.394, word_ins=7.565, length=4.459, ppl=213.2, wps=667.9, ups=1.62, wpb=413.2, bsz=18.1, num_updates=160000, lr=7.90569e-05, gnorm=0.681, clip=0, loss_scale=32768, train_wall=616, gb_free=13.3, wall=106866
2021-08-21 09:58:05 | INFO | train_inner | epoch 015:   3109 / 11278 loss=7.707, nll_loss=6.415, glat_accu=0, glat_context_p=0.393, word_ins=7.536, length=4.442, ppl=209, wps=633.9, ups=1.62, wpb=390.3, bsz=17, num_updates=161000, lr=7.8811e-05, gnorm=0.716, clip=0.2, loss_scale=32768, train_wall=613, gb_free=13.8, wall=107482
2021-08-21 10:08:26 | INFO | train_inner | epoch 015:   4109 / 11278 loss=7.695, nll_loss=6.401, glat_accu=0, glat_context_p=0.392, word_ins=7.524, length=4.448, ppl=207.24, wps=642.7, ups=1.61, wpb=399.1, bsz=17.4, num_updates=162000, lr=7.85674e-05, gnorm=0.693, clip=0, loss_scale=32768, train_wall=618, gb_free=13.4, wall=108103
2021-08-21 10:18:50 | INFO | train_inner | epoch 015:   5109 / 11278 loss=7.74, nll_loss=6.451, glat_accu=0, glat_context_p=0.392, word_ins=7.568, length=4.489, ppl=213.85, wps=673, ups=1.6, wpb=419.8, bsz=18.2, num_updates=163000, lr=7.8326e-05, gnorm=0.687, clip=0, loss_scale=32768, train_wall=621, gb_free=12.8, wall=108727
2021-08-21 10:27:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-21 10:29:15 | INFO | train_inner | epoch 015:   6110 / 11278 loss=7.76, nll_loss=6.473, glat_accu=0, glat_context_p=0.391, word_ins=7.587, length=4.494, ppl=216.73, wps=664.1, ups=1.6, wpb=415, bsz=17.8, num_updates=164000, lr=7.80869e-05, gnorm=0.686, clip=0, loss_scale=32768, train_wall=623, gb_free=13.2, wall=109352
2021-08-21 10:39:37 | INFO | train_inner | epoch 015:   7110 / 11278 loss=7.745, nll_loss=6.46, glat_accu=0, glat_context_p=0.39, word_ins=7.576, length=4.389, ppl=214.5, wps=648.3, ups=1.61, wpb=403.4, bsz=17.4, num_updates=165000, lr=7.78499e-05, gnorm=0.708, clip=0.1, loss_scale=32768, train_wall=620, gb_free=13.3, wall=109974
2021-08-21 10:50:01 | INFO | train_inner | epoch 015:   8110 / 11278 loss=7.731, nll_loss=6.44, glat_accu=0, glat_context_p=0.39, word_ins=7.558, length=4.496, ppl=212.52, wps=639, ups=1.6, wpb=398.4, bsz=17.3, num_updates=166000, lr=7.76151e-05, gnorm=0.689, clip=0, loss_scale=32768, train_wall=621, gb_free=13.6, wall=110597
2021-08-21 11:00:26 | INFO | train_inner | epoch 015:   9110 / 11278 loss=7.743, nll_loss=6.454, glat_accu=0, glat_context_p=0.389, word_ins=7.57, length=4.465, ppl=214.18, wps=635.9, ups=1.6, wpb=397.5, bsz=17.2, num_updates=167000, lr=7.73823e-05, gnorm=0.708, clip=0.1, loss_scale=32768, train_wall=623, gb_free=13.1, wall=111222
2021-08-21 11:10:46 | INFO | train_inner | epoch 015:  10110 / 11278 loss=7.728, nll_loss=6.438, glat_accu=0, glat_context_p=0.388, word_ins=7.556, length=4.447, ppl=212, wps=627.8, ups=1.61, wpb=389.7, bsz=16.9, num_updates=168000, lr=7.71517e-05, gnorm=0.694, clip=0, loss_scale=32768, train_wall=618, gb_free=13.5, wall=111843
2021-08-21 11:21:04 | INFO | train_inner | epoch 015:  11110 / 11278 loss=7.713, nll_loss=6.424, glat_accu=0, glat_context_p=0.388, word_ins=7.543, length=4.372, ppl=209.76, wps=667.6, ups=1.62, wpb=412.5, bsz=18.2, num_updates=169000, lr=7.69231e-05, gnorm=0.687, clip=0, loss_scale=32768, train_wall=616, gb_free=14.2, wall=112461
2021-08-21 11:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 11:30:05 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.215 | nll_loss 6.913 | word_ins 7.997 | length 4.372 | ppl 297.24 | bleu 0 | wps 567.6 | wpb 184.6 | bsz 8 | num_updates 169168 | best_bleu 0
2021-08-21 11:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 169168 updates
2021-08-21 11:30:05 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint15.pt
2021-08-21 11:30:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint15.pt
2021-08-21 11:30:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint15.pt (epoch 15 @ 169168 updates, score 0.0) (writing took 19.2244721170282 seconds)
2021-08-21 11:30:24 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-08-21 11:30:24 | INFO | train | epoch 015 | loss 7.731 | nll_loss 6.442 | glat_accu 0 | glat_context_p 0.391 | word_ins 7.559 | length 4.446 | ppl 212.42 | wps 610.4 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 169168 | lr 7.68849e-05 | gnorm 0.694 | clip 0 | loss_scale 32768 | train_wall 6965 | gb_free 13.6 | wall 113021
2021-08-21 11:30:24 | INFO | fairseq.trainer | begin training epoch 16
2021-08-21 11:30:24 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 11:39:04 | INFO | train_inner | epoch 016:    832 / 11278 loss=7.753, nll_loss=6.469, glat_accu=0, glat_context_p=0.387, word_ins=7.583, length=4.402, ppl=215.75, wps=380.6, ups=0.93, wpb=410.9, bsz=17.7, num_updates=170000, lr=7.66965e-05, gnorm=0.685, clip=0, loss_scale=32768, train_wall=611, gb_free=12.5, wall=113541
2021-08-21 11:49:23 | INFO | train_inner | epoch 016:   1832 / 11278 loss=7.703, nll_loss=6.41, glat_accu=0, glat_context_p=0.386, word_ins=7.532, length=4.426, ppl=208.36, wps=652.5, ups=1.61, wpb=404.3, bsz=17.7, num_updates=171000, lr=7.64719e-05, gnorm=0.695, clip=0, loss_scale=32768, train_wall=617, gb_free=13, wall=114160
2021-08-21 11:59:34 | INFO | train_inner | epoch 016:   2832 / 11278 loss=7.718, nll_loss=6.425, glat_accu=0, glat_context_p=0.386, word_ins=7.546, length=4.443, ppl=210.48, wps=636.2, ups=1.64, wpb=388.3, bsz=17.1, num_updates=172000, lr=7.62493e-05, gnorm=0.717, clip=0, loss_scale=32768, train_wall=608, gb_free=14.4, wall=114770
2021-08-21 12:09:57 | INFO | train_inner | epoch 016:   3832 / 11278 loss=7.724, nll_loss=6.432, glat_accu=0, glat_context_p=0.385, word_ins=7.551, length=4.468, ppl=211.43, wps=642.8, ups=1.6, wpb=400.8, bsz=17.4, num_updates=173000, lr=7.60286e-05, gnorm=0.693, clip=0, loss_scale=32768, train_wall=621, gb_free=14.1, wall=115394
2021-08-21 12:20:20 | INFO | train_inner | epoch 016:   4832 / 11278 loss=7.755, nll_loss=6.468, glat_accu=0, glat_context_p=0.384, word_ins=7.583, length=4.446, ppl=216.03, wps=643.3, ups=1.61, wpb=400.2, bsz=17.4, num_updates=174000, lr=7.58098e-05, gnorm=0.734, clip=0.3, loss_scale=32768, train_wall=620, gb_free=13.8, wall=116016
2021-08-21 12:30:40 | INFO | train_inner | epoch 016:   5832 / 11278 loss=7.758, nll_loss=6.47, glat_accu=0, glat_context_p=0.384, word_ins=7.585, length=4.468, ppl=216.46, wps=658.1, ups=1.61, wpb=408.7, bsz=17.6, num_updates=175000, lr=7.55929e-05, gnorm=0.701, clip=0, loss_scale=32768, train_wall=619, gb_free=13.4, wall=116637
2021-08-21 12:41:08 | INFO | train_inner | epoch 016:   6832 / 11278 loss=7.753, nll_loss=6.464, glat_accu=0, glat_context_p=0.383, word_ins=7.579, length=4.476, ppl=215.72, wps=635, ups=1.59, wpb=398.3, bsz=17.3, num_updates=176000, lr=7.53778e-05, gnorm=0.72, clip=0.1, loss_scale=32768, train_wall=625, gb_free=13.7, wall=117264
2021-08-21 12:51:33 | INFO | train_inner | epoch 016:   7832 / 11278 loss=7.732, nll_loss=6.44, glat_accu=0, glat_context_p=0.382, word_ins=7.558, length=4.465, ppl=212.57, wps=640.8, ups=1.6, wpb=400.4, bsz=17.3, num_updates=177000, lr=7.51646e-05, gnorm=0.705, clip=0, loss_scale=32768, train_wall=622, gb_free=13.4, wall=117889
2021-08-21 13:01:53 | INFO | train_inner | epoch 016:   8832 / 11278 loss=7.726, nll_loss=6.437, glat_accu=0, glat_context_p=0.382, word_ins=7.555, length=4.401, ppl=211.76, wps=662.6, ups=1.61, wpb=410.9, bsz=17.9, num_updates=178000, lr=7.49532e-05, gnorm=0.691, clip=0, loss_scale=32768, train_wall=618, gb_free=13.4, wall=118509
2021-08-21 13:12:16 | INFO | train_inner | epoch 016:   9832 / 11278 loss=7.738, nll_loss=6.448, glat_accu=0, glat_context_p=0.381, word_ins=7.565, length=4.443, ppl=213.5, wps=648.2, ups=1.6, wpb=404, bsz=17.6, num_updates=179000, lr=7.47435e-05, gnorm=0.694, clip=0, loss_scale=32768, train_wall=621, gb_free=13.4, wall=119133
2021-08-21 13:22:38 | INFO | train_inner | epoch 016:  10832 / 11278 loss=7.745, nll_loss=6.455, glat_accu=0, glat_context_p=0.38, word_ins=7.571, length=4.452, ppl=214.46, wps=654, ups=1.61, wpb=406.9, bsz=17.6, num_updates=180000, lr=7.45356e-05, gnorm=0.685, clip=0, loss_scale=32768, train_wall=620, gb_free=14.3, wall=119755
2021-08-21 13:25:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-21 13:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 13:35:04 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.342 | nll_loss 7.048 | word_ins 8.124 | length 4.371 | ppl 324.5 | bleu 0 | wps 543.8 | wpb 184.6 | bsz 8 | num_updates 180445 | best_bleu 0
2021-08-21 13:35:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 180445 updates
2021-08-21 13:35:04 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint16.pt
2021-08-21 13:35:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint16.pt
2021-08-21 13:35:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint16.pt (epoch 16 @ 180445 updates, score 0.0) (writing took 19.06760780804325 seconds)
2021-08-21 13:35:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-08-21 13:35:23 | INFO | train | epoch 016 | loss 7.734 | nll_loss 6.444 | glat_accu 0 | glat_context_p 0.383 | word_ins 7.562 | length 4.446 | ppl 212.95 | wps 607.1 | ups 1.5 | wpb 403.7 | bsz 17.5 | num_updates 180445 | lr 7.44436e-05 | gnorm 0.701 | clip 0 | loss_scale 32768 | train_wall 6986 | gb_free 13.7 | wall 120519
2021-08-21 13:35:23 | INFO | fairseq.trainer | begin training epoch 17
2021-08-21 13:35:23 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 13:41:05 | INFO | train_inner | epoch 017:    555 / 11278 loss=7.722, nll_loss=6.43, glat_accu=0, glat_context_p=0.38, word_ins=7.549, length=4.431, ppl=211.11, wps=372.9, ups=0.9, wpb=412.9, bsz=17.8, num_updates=181000, lr=7.43294e-05, gnorm=0.685, clip=0, loss_scale=32768, train_wall=619, gb_free=13.3, wall=120862
2021-08-21 13:51:25 | INFO | train_inner | epoch 017:   1555 / 11278 loss=7.717, nll_loss=6.424, glat_accu=0, glat_context_p=0.379, word_ins=7.544, length=4.43, ppl=210.4, wps=642.1, ups=1.61, wpb=397.7, bsz=17.3, num_updates=182000, lr=7.41249e-05, gnorm=0.713, clip=0, loss_scale=32768, train_wall=617, gb_free=13.6, wall=121482
2021-08-21 14:01:49 | INFO | train_inner | epoch 017:   2555 / 11278 loss=7.731, nll_loss=6.44, glat_accu=0, glat_context_p=0.378, word_ins=7.559, length=4.427, ppl=212.44, wps=644.2, ups=1.6, wpb=402.4, bsz=17.5, num_updates=183000, lr=7.39221e-05, gnorm=0.742, clip=0.4, loss_scale=32768, train_wall=622, gb_free=13.2, wall=122106
2021-08-21 14:12:03 | INFO | train_inner | epoch 017:   3555 / 11278 loss=7.719, nll_loss=6.428, glat_accu=0, glat_context_p=0.378, word_ins=7.547, length=4.392, ppl=210.64, wps=649, ups=1.63, wpb=398.5, bsz=17.5, num_updates=184000, lr=7.3721e-05, gnorm=0.707, clip=0, loss_scale=32768, train_wall=611, gb_free=13.6, wall=122720
2021-08-21 14:22:11 | INFO | train_inner | epoch 017:   4555 / 11278 loss=7.734, nll_loss=6.439, glat_accu=0, glat_context_p=0.377, word_ins=7.557, length=4.523, ppl=212.85, wps=691.6, ups=1.65, wpb=420.3, bsz=18.4, num_updates=185000, lr=7.35215e-05, gnorm=0.687, clip=0, loss_scale=32768, train_wall=605, gb_free=13, wall=123328
2021-08-21 14:32:39 | INFO | train_inner | epoch 017:   5555 / 11278 loss=7.732, nll_loss=6.438, glat_accu=0, glat_context_p=0.376, word_ins=7.556, length=4.514, ppl=212.63, wps=644.4, ups=1.59, wpb=404.4, bsz=17.6, num_updates=186000, lr=7.33236e-05, gnorm=0.708, clip=0, loss_scale=32768, train_wall=625, gb_free=13.8, wall=123955
2021-08-21 14:43:02 | INFO | train_inner | epoch 017:   6555 / 11278 loss=7.744, nll_loss=6.455, glat_accu=0, glat_context_p=0.376, word_ins=7.572, length=4.425, ppl=214.44, wps=648.1, ups=1.6, wpb=403.9, bsz=17.5, num_updates=187000, lr=7.31272e-05, gnorm=0.698, clip=0, loss_scale=32768, train_wall=621, gb_free=13.7, wall=124579
2021-08-21 14:53:26 | INFO | train_inner | epoch 017:   7555 / 11278 loss=7.743, nll_loss=6.452, glat_accu=0, glat_context_p=0.375, word_ins=7.569, length=4.455, ppl=214.19, wps=660.2, ups=1.6, wpb=411.7, bsz=17.8, num_updates=188000, lr=7.29325e-05, gnorm=0.703, clip=0, loss_scale=32768, train_wall=621, gb_free=13.4, wall=125202
2021-08-21 15:03:46 | INFO | train_inner | epoch 017:   8555 / 11278 loss=7.746, nll_loss=6.457, glat_accu=0, glat_context_p=0.374, word_ins=7.573, length=4.418, ppl=214.71, wps=630, ups=1.61, wpb=391, bsz=16.9, num_updates=189000, lr=7.27393e-05, gnorm=0.703, clip=0, loss_scale=32768, train_wall=618, gb_free=13.5, wall=125823
2021-08-21 15:14:01 | INFO | train_inner | epoch 017:   9555 / 11278 loss=7.742, nll_loss=6.45, glat_accu=0, glat_context_p=0.374, word_ins=7.567, length=4.451, ppl=214.01, wps=643.8, ups=1.63, wpb=396.1, bsz=17.1, num_updates=190000, lr=7.25476e-05, gnorm=0.708, clip=0, loss_scale=32768, train_wall=613, gb_free=13.4, wall=126438
2021-08-21 15:24:20 | INFO | train_inner | epoch 017:  10555 / 11278 loss=7.726, nll_loss=6.431, glat_accu=0, glat_context_p=0.373, word_ins=7.551, length=4.473, ppl=211.76, wps=653.1, ups=1.62, wpb=403.9, bsz=17.6, num_updates=191000, lr=7.23575e-05, gnorm=0.699, clip=0, loss_scale=32768, train_wall=616, gb_free=12.8, wall=127057
2021-08-21 15:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 15:39:35 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.652 | nll_loss 7.308 | word_ins 8.433 | length 4.372 | ppl 402.27 | bleu 0 | wps 546.3 | wpb 184.6 | bsz 8 | num_updates 191723 | best_bleu 0
2021-08-21 15:39:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 191723 updates
2021-08-21 15:39:35 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint17.pt
2021-08-21 15:39:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint17.pt
2021-08-21 15:39:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint17.pt (epoch 17 @ 191723 updates, score 0.0) (writing took 20.901370342995506 seconds)
2021-08-21 15:39:56 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-08-21 15:39:56 | INFO | train | epoch 017 | loss 7.734 | nll_loss 6.443 | glat_accu 0 | glat_context_p 0.376 | word_ins 7.561 | length 4.446 | ppl 212.94 | wps 609.3 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 191723 | lr 7.22209e-05 | gnorm 0.705 | clip 0 | loss_scale 32768 | train_wall 6961 | gb_free 12.5 | wall 127993
2021-08-21 15:39:56 | INFO | fairseq.trainer | begin training epoch 18
2021-08-21 15:39:56 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 15:42:48 | INFO | train_inner | epoch 018:    277 / 11278 loss=7.748, nll_loss=6.459, glat_accu=0, glat_context_p=0.372, word_ins=7.575, length=4.416, ppl=214.94, wps=368.4, ups=0.9, wpb=408.3, bsz=17.7, num_updates=192000, lr=7.21688e-05, gnorm=0.691, clip=0, loss_scale=32768, train_wall=621, gb_free=14, wall=128165
2021-08-21 15:53:13 | INFO | train_inner | epoch 018:   1277 / 11278 loss=7.726, nll_loss=6.432, glat_accu=0, glat_context_p=0.372, word_ins=7.551, length=4.452, ppl=211.65, wps=658.4, ups=1.6, wpb=411, bsz=18, num_updates=193000, lr=7.19816e-05, gnorm=0.708, clip=0, loss_scale=32768, train_wall=622, gb_free=13.3, wall=128789
2021-08-21 16:03:33 | INFO | train_inner | epoch 018:   2277 / 11278 loss=7.741, nll_loss=6.449, glat_accu=0, glat_context_p=0.371, word_ins=7.566, length=4.461, ppl=213.96, wps=649.6, ups=1.61, wpb=403.1, bsz=17.4, num_updates=194000, lr=7.17958e-05, gnorm=0.725, clip=0, loss_scale=32768, train_wall=618, gb_free=13, wall=129410
2021-08-21 16:13:54 | INFO | train_inner | epoch 018:   3277 / 11278 loss=7.714, nll_loss=6.418, glat_accu=0, glat_context_p=0.37, word_ins=7.54, length=4.437, ppl=209.9, wps=637.1, ups=1.61, wpb=395.7, bsz=17.2, num_updates=195000, lr=7.16115e-05, gnorm=0.712, clip=0, loss_scale=32768, train_wall=619, gb_free=12.2, wall=130031
2021-08-21 16:24:10 | INFO | train_inner | epoch 018:   4277 / 11278 loss=7.71, nll_loss=6.414, glat_accu=0, glat_context_p=0.37, word_ins=7.535, length=4.462, ppl=209.41, wps=649.7, ups=1.62, wpb=400.2, bsz=17.4, num_updates=196000, lr=7.14286e-05, gnorm=0.716, clip=0, loss_scale=32768, train_wall=614, gb_free=12.9, wall=130647
2021-08-21 16:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-21 16:34:39 | INFO | train_inner | epoch 018:   5278 / 11278 loss=7.733, nll_loss=6.442, glat_accu=0, glat_context_p=0.369, word_ins=7.56, length=4.4, ppl=212.71, wps=654.9, ups=1.59, wpb=411.5, bsz=17.8, num_updates=197000, lr=7.1247e-05, gnorm=0.704, clip=0, loss_scale=32768, train_wall=626, gb_free=13.7, wall=131275
2021-08-21 16:45:04 | INFO | train_inner | epoch 018:   6278 / 11278 loss=7.732, nll_loss=6.441, glat_accu=0, glat_context_p=0.368, word_ins=7.56, length=4.378, ppl=212.56, wps=651.4, ups=1.6, wpb=407.1, bsz=17.8, num_updates=198000, lr=7.10669e-05, gnorm=0.7, clip=0, loss_scale=32768, train_wall=623, gb_free=13.7, wall=131900
2021-08-21 16:55:19 | INFO | train_inner | epoch 018:   7278 / 11278 loss=7.759, nll_loss=6.469, glat_accu=0, glat_context_p=0.368, word_ins=7.584, length=4.45, ppl=216.61, wps=642.3, ups=1.62, wpb=395.3, bsz=17.1, num_updates=199000, lr=7.08881e-05, gnorm=0.716, clip=0, loss_scale=32768, train_wall=613, gb_free=13.7, wall=132516
2021-08-21 17:05:44 | INFO | train_inner | epoch 018:   8278 / 11278 loss=7.744, nll_loss=6.452, glat_accu=0, glat_context_p=0.367, word_ins=7.569, length=4.461, ppl=214.37, wps=648.8, ups=1.6, wpb=405.4, bsz=17.6, num_updates=200000, lr=7.07107e-05, gnorm=0.693, clip=0, loss_scale=32768, train_wall=622, gb_free=13.6, wall=133141
2021-08-21 17:15:35 | INFO | train_inner | epoch 018:   9278 / 11278 loss=7.766, nll_loss=6.478, glat_accu=0, glat_context_p=0.366, word_ins=7.592, length=4.452, ppl=217.69, wps=668.3, ups=1.69, wpb=394.8, bsz=17, num_updates=201000, lr=7.05346e-05, gnorm=0.722, clip=0, loss_scale=32768, train_wall=588, gb_free=13.4, wall=133731
2021-08-21 17:25:57 | INFO | train_inner | epoch 018:  10278 / 11278 loss=7.702, nll_loss=6.404, glat_accu=0, glat_context_p=0.366, word_ins=7.527, length=4.455, ppl=208.23, wps=657.4, ups=1.61, wpb=409.1, bsz=17.8, num_updates=202000, lr=7.03598e-05, gnorm=0.697, clip=0, loss_scale=32768, train_wall=620, gb_free=13.3, wall=134354
2021-08-21 17:36:23 | INFO | train_inner | epoch 018:  11278 / 11278 loss=7.749, nll_loss=6.456, glat_accu=0, glat_context_p=0.365, word_ins=7.573, length=4.486, ppl=215.19, wps=652.6, ups=1.6, wpb=408.4, bsz=17.8, num_updates=203000, lr=7.01862e-05, gnorm=0.705, clip=0, loss_scale=32768, train_wall=623, gb_free=13.2, wall=134979
2021-08-21 17:36:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 17:44:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.181 | nll_loss 6.885 | word_ins 7.962 | length 4.372 | ppl 290.2 | bleu 0 | wps 547.1 | wpb 184.6 | bsz 8 | num_updates 203000 | best_bleu 0
2021-08-21 17:44:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 203000 updates
2021-08-21 17:44:06 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint18.pt
2021-08-21 17:44:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint18.pt
2021-08-21 17:44:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint18.pt (epoch 18 @ 203000 updates, score 0.0) (writing took 18.99599104997469 seconds)
2021-08-21 17:44:25 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-08-21 17:44:25 | INFO | train | epoch 018 | loss 7.734 | nll_loss 6.441 | glat_accu 0 | glat_context_p 0.368 | word_ins 7.56 | length 4.446 | ppl 212.92 | wps 609.5 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 203000 | lr 7.01862e-05 | gnorm 0.709 | clip 0 | loss_scale 32768 | train_wall 6959 | gb_free 13.2 | wall 135462
2021-08-21 17:44:25 | INFO | fairseq.trainer | begin training epoch 19
2021-08-21 17:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 17:54:52 | INFO | train_inner | epoch 019:   1000 / 11278 loss=7.752, nll_loss=6.462, glat_accu=0, glat_context_p=0.364, word_ins=7.577, length=4.444, ppl=215.55, wps=377.3, ups=0.9, wpb=418.5, bsz=17.9, num_updates=204000, lr=7.0014e-05, gnorm=0.707, clip=0, loss_scale=32768, train_wall=624, gb_free=13.6, wall=136088
2021-08-21 18:05:10 | INFO | train_inner | epoch 019:   2000 / 11278 loss=7.717, nll_loss=6.42, glat_accu=0, glat_context_p=0.364, word_ins=7.541, length=4.445, ppl=210.37, wps=654.8, ups=1.62, wpb=405, bsz=17.8, num_updates=205000, lr=6.9843e-05, gnorm=0.722, clip=0, loss_scale=32768, train_wall=616, gb_free=14.1, wall=136707
2021-08-21 18:15:37 | INFO | train_inner | epoch 019:   3000 / 11278 loss=7.741, nll_loss=6.445, glat_accu=0, glat_context_p=0.363, word_ins=7.563, length=4.512, ppl=213.96, wps=637.1, ups=1.6, wpb=399.3, bsz=17.3, num_updates=206000, lr=6.96733e-05, gnorm=0.731, clip=0, loss_scale=32768, train_wall=624, gb_free=14, wall=137334
2021-08-21 18:26:00 | INFO | train_inner | epoch 019:   4000 / 11278 loss=7.734, nll_loss=6.438, glat_accu=0, glat_context_p=0.362, word_ins=7.557, length=4.46, ppl=212.84, wps=659.4, ups=1.61, wpb=410.7, bsz=17.9, num_updates=207000, lr=6.95048e-05, gnorm=0.708, clip=0, loss_scale=32768, train_wall=620, gb_free=13.3, wall=137956
2021-08-21 18:36:18 | INFO | train_inner | epoch 019:   5000 / 11278 loss=7.75, nll_loss=6.459, glat_accu=0, glat_context_p=0.362, word_ins=7.575, length=4.425, ppl=215.26, wps=649.9, ups=1.62, wpb=402.1, bsz=17.3, num_updates=208000, lr=6.93375e-05, gnorm=0.72, clip=0, loss_scale=32768, train_wall=616, gb_free=13.2, wall=138575
2021-08-21 18:46:41 | INFO | train_inner | epoch 019:   6000 / 11278 loss=7.738, nll_loss=6.443, glat_accu=0, glat_context_p=0.361, word_ins=7.562, length=4.448, ppl=213.43, wps=651.6, ups=1.6, wpb=406, bsz=17.6, num_updates=209000, lr=6.91714e-05, gnorm=0.731, clip=0.2, loss_scale=32768, train_wall=621, gb_free=12.3, wall=139198
2021-08-21 18:56:58 | INFO | train_inner | epoch 019:   7000 / 11278 loss=7.695, nll_loss=6.399, glat_accu=0, glat_context_p=0.36, word_ins=7.522, length=4.38, ppl=207.29, wps=647.5, ups=1.62, wpb=399.3, bsz=17.6, num_updates=210000, lr=6.90066e-05, gnorm=0.71, clip=0, loss_scale=32768, train_wall=614, gb_free=13, wall=139815
2021-08-21 19:07:25 | INFO | train_inner | epoch 019:   8000 / 11278 loss=7.735, nll_loss=6.442, glat_accu=0, glat_context_p=0.36, word_ins=7.561, length=4.413, ppl=213.04, wps=647.2, ups=1.59, wpb=405.8, bsz=17.6, num_updates=211000, lr=6.88428e-05, gnorm=0.71, clip=0, loss_scale=32768, train_wall=624, gb_free=13.1, wall=140442
2021-08-21 19:17:47 | INFO | train_inner | epoch 019:   9000 / 11278 loss=7.717, nll_loss=6.42, glat_accu=0, glat_context_p=0.359, word_ins=7.541, length=4.45, ppl=210.38, wps=654.8, ups=1.61, wpb=406.8, bsz=17.8, num_updates=212000, lr=6.86803e-05, gnorm=0.712, clip=0, loss_scale=32768, train_wall=619, gb_free=12.3, wall=141063
2021-08-21 19:28:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-21 19:28:09 | INFO | train_inner | epoch 019:  10001 / 11278 loss=7.743, nll_loss=6.448, glat_accu=0, glat_context_p=0.358, word_ins=7.566, length=4.474, ppl=214.17, wps=629.1, ups=1.61, wpb=391.8, bsz=17, num_updates=213000, lr=6.85189e-05, gnorm=0.72, clip=0, loss_scale=32768, train_wall=620, gb_free=13.3, wall=141686
2021-08-21 19:38:30 | INFO | train_inner | epoch 019:  11001 / 11278 loss=7.754, nll_loss=6.461, glat_accu=0, glat_context_p=0.358, word_ins=7.577, length=4.467, ppl=215.82, wps=634.6, ups=1.61, wpb=394.2, bsz=17, num_updates=214000, lr=6.83586e-05, gnorm=0.717, clip=0, loss_scale=32768, train_wall=619, gb_free=13.7, wall=142307
2021-08-21 19:41:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 19:49:09 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.265 | nll_loss 6.997 | word_ins 8.046 | length 4.368 | ppl 307.57 | bleu 0 | wps 545 | wpb 184.6 | bsz 8 | num_updates 214277 | best_bleu 0
2021-08-21 19:49:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 214277 updates
2021-08-21 19:49:09 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint19.pt
2021-08-21 19:49:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint19.pt
2021-08-21 19:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint19.pt (epoch 19 @ 214277 updates, score 0.0) (writing took 18.90419638901949 seconds)
2021-08-21 19:49:28 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-08-21 19:49:28 | INFO | train | epoch 019 | loss 7.734 | nll_loss 6.44 | glat_accu 0 | glat_context_p 0.361 | word_ins 7.558 | length 4.446 | ppl 212.9 | wps 606.8 | ups 1.5 | wpb 403.7 | bsz 17.5 | num_updates 214277 | lr 6.83144e-05 | gnorm 0.717 | clip 0 | loss_scale 32768 | train_wall 6990 | gb_free 12.5 | wall 142964
2021-08-21 19:49:28 | INFO | fairseq.trainer | begin training epoch 20
2021-08-21 19:49:28 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 19:57:03 | INFO | train_inner | epoch 020:    723 / 11278 loss=7.743, nll_loss=6.447, glat_accu=0, glat_context_p=0.357, word_ins=7.565, length=4.488, ppl=214.25, wps=372.9, ups=0.9, wpb=414.9, bsz=18.1, num_updates=215000, lr=6.81994e-05, gnorm=0.711, clip=0, loss_scale=32768, train_wall=626, gb_free=13.1, wall=143420
2021-08-21 20:07:11 | INFO | train_inner | epoch 020:   1723 / 11278 loss=7.709, nll_loss=6.409, glat_accu=0, glat_context_p=0.356, word_ins=7.531, length=4.453, ppl=209.18, wps=659.3, ups=1.65, wpb=400.5, bsz=17.6, num_updates=216000, lr=6.80414e-05, gnorm=0.727, clip=0, loss_scale=32768, train_wall=605, gb_free=13, wall=144027
2021-08-21 20:17:16 | INFO | train_inner | epoch 020:   2723 / 11278 loss=7.714, nll_loss=6.415, glat_accu=0, glat_context_p=0.356, word_ins=7.537, length=4.452, ppl=210.02, wps=656.8, ups=1.65, wpb=397.4, bsz=17.4, num_updates=217000, lr=6.78844e-05, gnorm=0.731, clip=0, loss_scale=32768, train_wall=603, gb_free=13, wall=144632
2021-08-21 20:27:37 | INFO | train_inner | epoch 020:   3723 / 11278 loss=7.744, nll_loss=6.449, glat_accu=0, glat_context_p=0.355, word_ins=7.567, length=4.465, ppl=214.43, wps=640.9, ups=1.61, wpb=398, bsz=17.3, num_updates=218000, lr=6.77285e-05, gnorm=0.732, clip=0, loss_scale=32768, train_wall=619, gb_free=13.4, wall=145253
2021-08-21 20:38:01 | INFO | train_inner | epoch 020:   4723 / 11278 loss=7.719, nll_loss=6.423, glat_accu=0, glat_context_p=0.354, word_ins=7.544, length=4.417, ppl=210.74, wps=666.1, ups=1.6, wpb=415.6, bsz=18.2, num_updates=219000, lr=6.75737e-05, gnorm=0.717, clip=0, loss_scale=32768, train_wall=621, gb_free=13.8, wall=145877
2021-08-21 20:48:21 | INFO | train_inner | epoch 020:   5723 / 11278 loss=7.742, nll_loss=6.448, glat_accu=0, glat_context_p=0.354, word_ins=7.566, length=4.417, ppl=214.02, wps=660.1, ups=1.61, wpb=409.8, bsz=17.6, num_updates=220000, lr=6.742e-05, gnorm=0.716, clip=0.1, loss_scale=32768, train_wall=618, gb_free=13.8, wall=146498
2021-08-21 20:58:47 | INFO | train_inner | epoch 020:   6723 / 11278 loss=7.73, nll_loss=6.432, glat_accu=0, glat_context_p=0.353, word_ins=7.552, length=4.482, ppl=212.32, wps=646.3, ups=1.6, wpb=404.1, bsz=17.5, num_updates=221000, lr=6.72673e-05, gnorm=0.716, clip=0, loss_scale=32768, train_wall=623, gb_free=13.3, wall=147123
2021-08-21 21:09:04 | INFO | train_inner | epoch 020:   7723 / 11278 loss=7.71, nll_loss=6.411, glat_accu=0, glat_context_p=0.352, word_ins=7.532, length=4.453, ppl=209.41, wps=634.9, ups=1.62, wpb=392.2, bsz=17.2, num_updates=222000, lr=6.71156e-05, gnorm=0.727, clip=0, loss_scale=32768, train_wall=615, gb_free=13.6, wall=147741
2021-08-21 21:19:24 | INFO | train_inner | epoch 020:   8723 / 11278 loss=7.736, nll_loss=6.441, glat_accu=0, glat_context_p=0.352, word_ins=7.56, length=4.419, ppl=213.17, wps=639.6, ups=1.61, wpb=396.2, bsz=17.2, num_updates=223000, lr=6.6965e-05, gnorm=0.735, clip=0, loss_scale=32768, train_wall=617, gb_free=13.6, wall=148361
2021-08-21 21:29:47 | INFO | train_inner | epoch 020:   9723 / 11278 loss=7.765, nll_loss=6.47, glat_accu=0, glat_context_p=0.351, word_ins=7.585, length=4.525, ppl=217.59, wps=644.6, ups=1.61, wpb=401.5, bsz=17.2, num_updates=224000, lr=6.68153e-05, gnorm=0.719, clip=0, loss_scale=32768, train_wall=621, gb_free=13.4, wall=148983
2021-08-21 21:40:11 | INFO | train_inner | epoch 020:  10723 / 11278 loss=7.761, nll_loss=6.471, glat_accu=0, glat_context_p=0.35, word_ins=7.586, length=4.395, ppl=216.88, wps=662.5, ups=1.6, wpb=413.4, bsz=17.7, num_updates=225000, lr=6.66667e-05, gnorm=0.702, clip=0, loss_scale=32768, train_wall=622, gb_free=14.3, wall=149607
2021-08-21 21:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 21:53:44 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.416 | nll_loss 7.113 | word_ins 8.197 | length 4.373 | ppl 341.49 | bleu 0 | wps 544.3 | wpb 184.6 | bsz 8 | num_updates 225555 | best_bleu 0
2021-08-21 21:53:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 225555 updates
2021-08-21 21:53:44 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint20.pt
2021-08-21 21:53:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint20.pt
2021-08-21 21:54:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint20.pt (epoch 20 @ 225555 updates, score 0.0) (writing took 18.858799494977575 seconds)
2021-08-21 21:54:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-08-21 21:54:03 | INFO | train | epoch 020 | loss 7.733 | nll_loss 6.437 | glat_accu 0 | glat_context_p 0.353 | word_ins 7.556 | length 4.446 | ppl 212.8 | wps 609.1 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 225555 | lr 6.65846e-05 | gnorm 0.721 | clip 0 | loss_scale 32768 | train_wall 6963 | gb_free 12.6 | wall 150440
2021-08-21 21:54:03 | INFO | fairseq.trainer | begin training epoch 21
2021-08-21 21:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-21 21:58:37 | INFO | train_inner | epoch 021:    445 / 11278 loss=7.727, nll_loss=6.433, glat_accu=0, glat_context_p=0.35, word_ins=7.552, length=4.38, ppl=211.93, wps=362.6, ups=0.9, wpb=401.3, bsz=17.4, num_updates=226000, lr=6.6519e-05, gnorm=0.717, clip=0, loss_scale=32768, train_wall=619, gb_free=13.6, wall=150714
2021-08-21 22:09:01 | INFO | train_inner | epoch 021:   1445 / 11278 loss=7.729, nll_loss=6.431, glat_accu=0, glat_context_p=0.349, word_ins=7.551, length=4.465, ppl=212.2, wps=653.1, ups=1.6, wpb=407.1, bsz=17.7, num_updates=227000, lr=6.63723e-05, gnorm=0.736, clip=0, loss_scale=32768, train_wall=621, gb_free=12.7, wall=151337
2021-08-21 22:19:24 | INFO | train_inner | epoch 021:   2445 / 11278 loss=7.704, nll_loss=6.404, glat_accu=0, glat_context_p=0.348, word_ins=7.527, length=4.433, ppl=208.58, wps=659.2, ups=1.6, wpb=411.2, bsz=17.9, num_updates=228000, lr=6.62266e-05, gnorm=0.721, clip=0, loss_scale=32768, train_wall=621, gb_free=13.7, wall=151961
2021-08-21 22:29:48 | INFO | train_inner | epoch 021:   3445 / 11278 loss=7.747, nll_loss=6.451, glat_accu=0, glat_context_p=0.348, word_ins=7.569, length=4.453, ppl=214.8, wps=652.4, ups=1.6, wpb=407, bsz=17.5, num_updates=229000, lr=6.60819e-05, gnorm=0.719, clip=0, loss_scale=32768, train_wall=621, gb_free=13, wall=152585
2021-08-21 22:33:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-21 22:40:10 | INFO | train_inner | epoch 021:   4446 / 11278 loss=7.754, nll_loss=6.46, glat_accu=0, glat_context_p=0.347, word_ins=7.576, length=4.447, ppl=215.87, wps=645.2, ups=1.61, wpb=401.3, bsz=17.4, num_updates=230000, lr=6.5938e-05, gnorm=0.73, clip=0, loss_scale=32768, train_wall=620, gb_free=13.6, wall=153207
2021-08-21 22:50:33 | INFO | train_inner | epoch 021:   5446 / 11278 loss=7.725, nll_loss=6.429, glat_accu=0, glat_context_p=0.346, word_ins=7.549, length=4.404, ppl=211.59, wps=648.9, ups=1.61, wpb=404.1, bsz=17.6, num_updates=231000, lr=6.57952e-05, gnorm=0.718, clip=0, loss_scale=32768, train_wall=620, gb_free=13.8, wall=153830
2021-08-21 23:00:56 | INFO | train_inner | epoch 021:   6446 / 11278 loss=7.746, nll_loss=6.451, glat_accu=0, glat_context_p=0.346, word_ins=7.569, length=4.418, ppl=214.66, wps=645.9, ups=1.6, wpb=402.7, bsz=17.4, num_updates=232000, lr=6.56532e-05, gnorm=0.73, clip=0, loss_scale=32768, train_wall=621, gb_free=13.1, wall=154453
2021-08-21 23:10:50 | INFO | train_inner | epoch 021:   7446 / 11278 loss=7.727, nll_loss=6.425, glat_accu=0, glat_context_p=0.345, word_ins=7.546, length=4.533, ppl=211.9, wps=691.4, ups=1.69, wpb=410.2, bsz=17.8, num_updates=233000, lr=6.55122e-05, gnorm=0.726, clip=0, loss_scale=32768, train_wall=591, gb_free=13.6, wall=155046
2021-08-21 23:21:09 | INFO | train_inner | epoch 021:   8446 / 11278 loss=7.738, nll_loss=6.443, glat_accu=0, glat_context_p=0.344, word_ins=7.561, length=4.416, ppl=213.53, wps=651.9, ups=1.61, wpb=403.8, bsz=17.7, num_updates=234000, lr=6.5372e-05, gnorm=0.73, clip=0, loss_scale=32768, train_wall=617, gb_free=13.3, wall=155666
2021-08-21 23:31:36 | INFO | train_inner | epoch 021:   9446 / 11278 loss=7.736, nll_loss=6.439, glat_accu=0, glat_context_p=0.344, word_ins=7.558, length=4.434, ppl=213.15, wps=620.5, ups=1.6, wpb=388.8, bsz=17, num_updates=235000, lr=6.52328e-05, gnorm=0.754, clip=0, loss_scale=32768, train_wall=624, gb_free=12.1, wall=156293
2021-08-21 23:41:59 | INFO | train_inner | epoch 021:  10446 / 11278 loss=7.728, nll_loss=6.429, glat_accu=0, glat_context_p=0.343, word_ins=7.549, length=4.474, ppl=212.06, wps=645.6, ups=1.6, wpb=402.5, bsz=17.5, num_updates=236000, lr=6.50945e-05, gnorm=0.723, clip=0, loss_scale=32768, train_wall=621, gb_free=13.1, wall=156916
2021-08-21 23:50:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-21 23:58:16 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.365 | nll_loss 7.037 | word_ins 8.147 | length 4.366 | ppl 329.74 | bleu 0 | wps 545 | wpb 184.6 | bsz 8 | num_updates 236832 | best_bleu 0
2021-08-21 23:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 236832 updates
2021-08-21 23:58:16 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint21.pt
2021-08-21 23:58:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint21.pt
2021-08-21 23:58:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint21.pt (epoch 21 @ 236832 updates, score 0.0) (writing took 18.819036945991684 seconds)
2021-08-21 23:58:35 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-08-21 23:58:35 | INFO | train | epoch 021 | loss 7.734 | nll_loss 6.436 | glat_accu 0 | glat_context_p 0.346 | word_ins 7.556 | length 4.446 | ppl 212.84 | wps 609.4 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 236832 | lr 6.498e-05 | gnorm 0.728 | clip 0 | loss_scale 32768 | train_wall 6960 | gb_free 13.2 | wall 157912
2021-08-21 23:58:35 | INFO | fairseq.trainer | begin training epoch 22
2021-08-21 23:58:35 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-22 00:00:17 | INFO | train_inner | epoch 022:    168 / 11278 loss=7.727, nll_loss=6.432, glat_accu=0, glat_context_p=0.342, word_ins=7.551, length=4.399, ppl=211.9, wps=364.7, ups=0.91, wpb=400.3, bsz=17.4, num_updates=237000, lr=6.4957e-05, gnorm=0.724, clip=0, loss_scale=32768, train_wall=611, gb_free=13.4, wall=158014
2021-08-22 00:10:38 | INFO | train_inner | epoch 022:   1168 / 11278 loss=7.727, nll_loss=6.43, glat_accu=0, glat_context_p=0.342, word_ins=7.55, length=4.416, ppl=211.88, wps=654.7, ups=1.61, wpb=406.6, bsz=17.7, num_updates=238000, lr=6.48204e-05, gnorm=0.733, clip=0, loss_scale=32768, train_wall=618, gb_free=14.1, wall=158635
2021-08-22 00:20:57 | INFO | train_inner | epoch 022:   2168 / 11278 loss=7.705, nll_loss=6.402, glat_accu=0, glat_context_p=0.341, word_ins=7.525, length=4.476, ppl=208.61, wps=643.3, ups=1.62, wpb=398.1, bsz=17.4, num_updates=239000, lr=6.46846e-05, gnorm=0.738, clip=0, loss_scale=32768, train_wall=616, gb_free=13.9, wall=159254
2021-08-22 00:31:19 | INFO | train_inner | epoch 022:   3168 / 11278 loss=7.738, nll_loss=6.441, glat_accu=0, glat_context_p=0.34, word_ins=7.56, length=4.44, ppl=213.49, wps=661.8, ups=1.61, wpb=412, bsz=17.8, num_updates=240000, lr=6.45497e-05, gnorm=0.749, clip=0.2, loss_scale=32768, train_wall=620, gb_free=13.3, wall=159876
2021-08-22 00:41:42 | INFO | train_inner | epoch 022:   4168 / 11278 loss=7.726, nll_loss=6.428, glat_accu=0, glat_context_p=0.34, word_ins=7.549, length=4.405, ppl=211.65, wps=675.2, ups=1.61, wpb=420.4, bsz=18.2, num_updates=241000, lr=6.44157e-05, gnorm=0.737, clip=0, loss_scale=32768, train_wall=620, gb_free=13.5, wall=160499
2021-08-22 00:52:03 | INFO | train_inner | epoch 022:   5168 / 11278 loss=7.725, nll_loss=6.425, glat_accu=0, glat_context_p=0.339, word_ins=7.546, length=4.458, ppl=211.5, wps=652.1, ups=1.61, wpb=404.8, bsz=17.6, num_updates=242000, lr=6.42824e-05, gnorm=0.742, clip=0, loss_scale=32768, train_wall=618, gb_free=13.2, wall=161119
2021-08-22 01:02:21 | INFO | train_inner | epoch 022:   6168 / 11278 loss=7.717, nll_loss=6.417, glat_accu=0, glat_context_p=0.338, word_ins=7.538, length=4.457, ppl=210.35, wps=626.4, ups=1.62, wpb=387.1, bsz=17, num_updates=243000, lr=6.415e-05, gnorm=0.745, clip=0, loss_scale=32768, train_wall=616, gb_free=13.1, wall=161737
2021-08-22 01:12:46 | INFO | train_inner | epoch 022:   7168 / 11278 loss=7.728, nll_loss=6.433, glat_accu=0, glat_context_p=0.338, word_ins=7.552, length=4.398, ppl=212.08, wps=639.4, ups=1.6, wpb=400.1, bsz=17.5, num_updates=244000, lr=6.40184e-05, gnorm=0.741, clip=0, loss_scale=32768, train_wall=623, gb_free=13.3, wall=162363
2021-08-22 01:23:05 | INFO | train_inner | epoch 022:   8168 / 11278 loss=7.74, nll_loss=6.444, glat_accu=0, glat_context_p=0.337, word_ins=7.562, length=4.439, ppl=213.79, wps=637.5, ups=1.62, wpb=394.4, bsz=17.2, num_updates=245000, lr=6.38877e-05, gnorm=0.733, clip=0, loss_scale=32768, train_wall=616, gb_free=13.8, wall=162982
2021-08-22 01:30:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-22 01:33:29 | INFO | train_inner | epoch 022:   9169 / 11278 loss=7.743, nll_loss=6.448, glat_accu=0, glat_context_p=0.336, word_ins=7.566, length=4.434, ppl=214.24, wps=649, ups=1.6, wpb=404.6, bsz=17.5, num_updates=246000, lr=6.37577e-05, gnorm=0.736, clip=0, loss_scale=32768, train_wall=621, gb_free=13.9, wall=163605
2021-08-22 01:43:50 | INFO | train_inner | epoch 022:  10169 / 11278 loss=7.764, nll_loss=6.472, glat_accu=0, glat_context_p=0.336, word_ins=7.587, length=4.433, ppl=217.44, wps=675.1, ups=1.61, wpb=419.6, bsz=18, num_updates=247000, lr=6.36285e-05, gnorm=0.719, clip=0, loss_scale=32768, train_wall=619, gb_free=13.5, wall=164227
2021-08-22 01:54:15 | INFO | train_inner | epoch 022:  11169 / 11278 loss=7.758, nll_loss=6.46, glat_accu=0, glat_context_p=0.335, word_ins=7.577, length=4.535, ppl=216.46, wps=636.1, ups=1.6, wpb=397.7, bsz=17.2, num_updates=248000, lr=6.35001e-05, gnorm=0.749, clip=0, loss_scale=32768, train_wall=623, gb_free=13.1, wall=164852
2021-08-22 01:55:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-22 02:02:42 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.231 | nll_loss 6.971 | word_ins 8.012 | length 4.372 | ppl 300.36 | bleu 0 | wps 578.6 | wpb 184.6 | bsz 8 | num_updates 248109 | best_bleu 0
2021-08-22 02:02:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 248109 updates
2021-08-22 02:02:42 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint22.pt
2021-08-22 02:02:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint22.pt
2021-08-22 02:03:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint22.pt (epoch 22 @ 248109 updates, score 0.0) (writing took 20.737838282017037 seconds)
2021-08-22 02:03:03 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-08-22 02:03:03 | INFO | train | epoch 022 | loss 7.733 | nll_loss 6.435 | glat_accu 0 | glat_context_p 0.338 | word_ins 7.555 | length 4.445 | ppl 212.71 | wps 609.7 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 248109 | lr 6.34861e-05 | gnorm 0.739 | clip 0 | loss_scale 32768 | train_wall 6981 | gb_free 12.3 | wall 165379
2021-08-22 02:03:03 | INFO | fairseq.trainer | begin training epoch 23
2021-08-22 02:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-22 02:12:16 | INFO | train_inner | epoch 023:    891 / 11278 loss=7.713, nll_loss=6.414, glat_accu=0, glat_context_p=0.334, word_ins=7.536, length=4.418, ppl=209.81, wps=380.4, ups=0.93, wpb=411.1, bsz=17.8, num_updates=249000, lr=6.33724e-05, gnorm=0.739, clip=0, loss_scale=32768, train_wall=619, gb_free=14.3, wall=165933
2021-08-22 02:22:32 | INFO | train_inner | epoch 023:   1891 / 11278 loss=7.717, nll_loss=6.416, glat_accu=0, glat_context_p=0.334, word_ins=7.538, length=4.477, ppl=210.44, wps=658, ups=1.62, wpb=405.1, bsz=17.6, num_updates=250000, lr=6.32456e-05, gnorm=0.751, clip=0, loss_scale=32768, train_wall=613, gb_free=13.9, wall=166548
2021-08-22 02:33:01 | INFO | train_inner | epoch 023:   2891 / 11278 loss=7.735, nll_loss=6.434, glat_accu=0, glat_context_p=0.333, word_ins=7.553, length=4.443, ppl=212.99, wps=634.2, ups=1.59, wpb=399.3, bsz=17.3, num_updates=251000, lr=6.31194e-05, gnorm=0.75, clip=0, loss_scale=32768, train_wall=627, gb_free=13.1, wall=167178
2021-08-22 02:43:21 | INFO | train_inner | epoch 023:   3891 / 11278 loss=7.732, nll_loss=6.43, glat_accu=0, glat_context_p=0.332, word_ins=7.551, length=4.431, ppl=212.54, wps=641, ups=1.61, wpb=397.5, bsz=17.3, num_updates=252000, lr=6.29941e-05, gnorm=0.745, clip=0, loss_scale=32768, train_wall=618, gb_free=13.2, wall=167798
2021-08-22 02:53:46 | INFO | train_inner | epoch 023:   4891 / 11278 loss=7.726, nll_loss=6.425, glat_accu=0, glat_context_p=0.332, word_ins=7.546, length=4.406, ppl=211.65, wps=654, ups=1.6, wpb=408.7, bsz=17.9, num_updates=253000, lr=6.28695e-05, gnorm=0.742, clip=0, loss_scale=32768, train_wall=623, gb_free=13.4, wall=168423
2021-08-22 03:04:07 | INFO | train_inner | epoch 023:   5891 / 11278 loss=7.716, nll_loss=6.41, glat_accu=0, glat_context_p=0.331, word_ins=7.533, length=4.466, ppl=210.19, wps=630.2, ups=1.61, wpb=391.3, bsz=17.1, num_updates=254000, lr=6.27456e-05, gnorm=0.749, clip=0, loss_scale=32768, train_wall=618, gb_free=13, wall=169044
2021-08-22 03:14:29 | INFO | train_inner | epoch 023:   6891 / 11278 loss=7.735, nll_loss=6.433, glat_accu=0, glat_context_p=0.33, word_ins=7.553, length=4.451, ppl=212.98, wps=651.2, ups=1.61, wpb=405, bsz=17.6, num_updates=255000, lr=6.26224e-05, gnorm=0.738, clip=0, loss_scale=32768, train_wall=619, gb_free=13.1, wall=169666
2021-08-22 03:24:53 | INFO | train_inner | epoch 023:   7891 / 11278 loss=7.75, nll_loss=6.451, glat_accu=0, glat_context_p=0.33, word_ins=7.569, length=4.438, ppl=215.25, wps=648.4, ups=1.6, wpb=404.5, bsz=17.6, num_updates=256000, lr=6.25e-05, gnorm=0.755, clip=0.1, loss_scale=32768, train_wall=621, gb_free=12.8, wall=170290
2021-08-22 03:35:13 | INFO | train_inner | epoch 023:   8891 / 11278 loss=7.726, nll_loss=6.424, glat_accu=0, glat_context_p=0.329, word_ins=7.545, length=4.445, ppl=211.76, wps=645.9, ups=1.61, wpb=400.3, bsz=17.3, num_updates=257000, lr=6.23783e-05, gnorm=0.744, clip=0, loss_scale=32768, train_wall=617, gb_free=13.5, wall=170910
2021-08-22 03:45:33 | INFO | train_inner | epoch 023:   9891 / 11278 loss=7.742, nll_loss=6.444, glat_accu=0, glat_context_p=0.328, word_ins=7.562, length=4.407, ppl=214.05, wps=667.7, ups=1.61, wpb=414.3, bsz=18, num_updates=258000, lr=6.22573e-05, gnorm=0.726, clip=0, loss_scale=32768, train_wall=618, gb_free=13.1, wall=171530
2021-08-22 03:55:57 | INFO | train_inner | epoch 023:  10891 / 11278 loss=7.758, nll_loss=6.457, glat_accu=0, glat_context_p=0.328, word_ins=7.574, length=4.509, ppl=216.48, wps=652.2, ups=1.6, wpb=406.7, bsz=17.6, num_updates=259000, lr=6.2137e-05, gnorm=0.734, clip=0, loss_scale=32768, train_wall=621, gb_free=12.5, wall=172154
2021-08-22 03:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-22 04:07:40 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.794 | nll_loss 7.683 | word_ins 8.575 | length 4.371 | ppl 443.89 | bleu 0 | wps 549.4 | wpb 184.6 | bsz 8 | num_updates 259387 | best_bleu 0
2021-08-22 04:07:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 259387 updates
2021-08-22 04:07:40 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint23.pt
2021-08-22 04:07:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint23.pt
2021-08-22 04:07:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint23.pt (epoch 23 @ 259387 updates, score 0.0) (writing took 18.114518144982867 seconds)
2021-08-22 04:07:58 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-08-22 04:07:58 | INFO | train | epoch 023 | loss 7.733 | nll_loss 6.433 | glat_accu 0 | glat_context_p 0.331 | word_ins 7.553 | length 4.445 | ppl 212.79 | wps 607.5 | ups 1.5 | wpb 403.7 | bsz 17.5 | num_updates 259387 | lr 6.20906e-05 | gnorm 0.743 | clip 0 | loss_scale 32768 | train_wall 6988 | gb_free 13.4 | wall 172874
2021-08-22 04:07:58 | INFO | fairseq.trainer | begin training epoch 24
2021-08-22 04:07:58 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-22 04:14:16 | INFO | train_inner | epoch 024:    613 / 11278 loss=7.735, nll_loss=6.433, glat_accu=0, glat_context_p=0.327, word_ins=7.553, length=4.462, ppl=213.04, wps=360.8, ups=0.91, wpb=396.6, bsz=17.2, num_updates=260000, lr=6.20174e-05, gnorm=0.742, clip=0, loss_scale=32768, train_wall=617, gb_free=13.5, wall=173253
2021-08-22 04:24:41 | INFO | train_inner | epoch 024:   1613 / 11278 loss=7.732, nll_loss=6.43, glat_accu=0, glat_context_p=0.326, word_ins=7.55, length=4.466, ppl=212.64, wps=644.8, ups=1.6, wpb=402.9, bsz=17.6, num_updates=261000, lr=6.18984e-05, gnorm=0.75, clip=0, loss_scale=32768, train_wall=622, gb_free=13.1, wall=173878
2021-08-22 04:34:59 | INFO | train_inner | epoch 024:   2613 / 11278 loss=7.727, nll_loss=6.426, glat_accu=0, glat_context_p=0.326, word_ins=7.547, length=4.43, ppl=211.9, wps=647.7, ups=1.62, wpb=400.6, bsz=17.3, num_updates=262000, lr=6.17802e-05, gnorm=0.777, clip=0.1, loss_scale=32768, train_wall=616, gb_free=13.6, wall=174496
2021-08-22 04:36:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-22 04:45:24 | INFO | train_inner | epoch 024:   3614 / 11278 loss=7.722, nll_loss=6.419, glat_accu=0, glat_context_p=0.325, word_ins=7.541, length=4.433, ppl=211.07, wps=639.5, ups=1.6, wpb=399.6, bsz=17.4, num_updates=263000, lr=6.16626e-05, gnorm=0.752, clip=0, loss_scale=32768, train_wall=622, gb_free=13.8, wall=175121
2021-08-22 04:55:31 | INFO | train_inner | epoch 024:   4614 / 11278 loss=7.744, nll_loss=6.444, glat_accu=0, glat_context_p=0.324, word_ins=7.562, length=4.445, ppl=214.31, wps=686.3, ups=1.65, wpb=416.3, bsz=18, num_updates=264000, lr=6.15457e-05, gnorm=0.746, clip=0, loss_scale=32768, train_wall=604, gb_free=13.3, wall=175728
2021-08-22 05:05:47 | INFO | train_inner | epoch 024:   5614 / 11278 loss=7.75, nll_loss=6.45, glat_accu=0, glat_context_p=0.324, word_ins=7.568, length=4.457, ppl=215.28, wps=657.6, ups=1.62, wpb=405.3, bsz=17.4, num_updates=265000, lr=6.14295e-05, gnorm=0.753, clip=0, loss_scale=32768, train_wall=614, gb_free=13.4, wall=176344
2021-08-22 05:16:08 | INFO | train_inner | epoch 024:   6614 / 11278 loss=7.726, nll_loss=6.424, glat_accu=0, glat_context_p=0.323, word_ins=7.545, length=4.446, ppl=211.77, wps=671.8, ups=1.61, wpb=417.3, bsz=18.1, num_updates=266000, lr=6.13139e-05, gnorm=0.736, clip=0, loss_scale=32768, train_wall=619, gb_free=13.6, wall=176965
2021-08-22 05:26:31 | INFO | train_inner | epoch 024:   7614 / 11278 loss=7.729, nll_loss=6.427, glat_accu=0, glat_context_p=0.322, word_ins=7.548, length=4.434, ppl=212.18, wps=640.3, ups=1.61, wpb=398.5, bsz=17.3, num_updates=267000, lr=6.1199e-05, gnorm=0.751, clip=0, loss_scale=32768, train_wall=620, gb_free=13, wall=177587
2021-08-22 05:36:56 | INFO | train_inner | epoch 024:   8614 / 11278 loss=7.749, nll_loss=6.449, glat_accu=0, glat_context_p=0.322, word_ins=7.567, length=4.454, ppl=215.11, wps=643, ups=1.6, wpb=402.2, bsz=17.4, num_updates=268000, lr=6.10847e-05, gnorm=0.749, clip=0, loss_scale=32768, train_wall=623, gb_free=12.3, wall=178213
2021-08-22 05:47:13 | INFO | train_inner | epoch 024:   9614 / 11278 loss=7.715, nll_loss=6.412, glat_accu=0, glat_context_p=0.321, word_ins=7.535, length=4.415, ppl=210.16, wps=657.7, ups=1.62, wpb=405.7, bsz=17.7, num_updates=269000, lr=6.09711e-05, gnorm=0.749, clip=0, loss_scale=32768, train_wall=614, gb_free=13.8, wall=178830
2021-08-22 05:57:33 | INFO | train_inner | epoch 024:  10614 / 11278 loss=7.727, nll_loss=6.424, glat_accu=0, glat_context_p=0.32, word_ins=7.545, length=4.468, ppl=211.91, wps=639.8, ups=1.61, wpb=396.8, bsz=17.3, num_updates=270000, lr=6.08581e-05, gnorm=0.755, clip=0, loss_scale=32768, train_wall=618, gb_free=14.2, wall=179450
2021-08-22 06:04:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-22 06:12:09 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.29 | nll_loss 7.002 | word_ins 8.072 | length 4.369 | ppl 313.03 | bleu 0 | wps 550.9 | wpb 184.6 | bsz 8 | num_updates 270664 | best_bleu 0
2021-08-22 06:12:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 270664 updates
2021-08-22 06:12:09 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint24.pt
2021-08-22 06:12:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint24.pt
2021-08-22 06:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint24.pt (epoch 24 @ 270664 updates, score 0.0) (writing took 18.395817275042646 seconds)
2021-08-22 06:12:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-08-22 06:12:27 | INFO | train | epoch 024 | loss 7.733 | nll_loss 6.431 | glat_accu 0 | glat_context_p 0.323 | word_ins 7.551 | length 4.445 | ppl 212.69 | wps 609.6 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 270664 | lr 6.07834e-05 | gnorm 0.751 | clip 0 | loss_scale 32768 | train_wall 6963 | gb_free 14 | wall 180344
2021-08-22 06:12:27 | INFO | fairseq.trainer | begin training epoch 25
2021-08-22 06:12:27 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-22 06:15:55 | INFO | train_inner | epoch 025:    336 / 11278 loss=7.74, nll_loss=6.438, glat_accu=0, glat_context_p=0.32, word_ins=7.558, length=4.454, ppl=213.71, wps=367, ups=0.91, wpb=404.2, bsz=17.6, num_updates=271000, lr=6.07457e-05, gnorm=0.749, clip=0, loss_scale=32768, train_wall=619, gb_free=13.5, wall=180551
2021-08-22 06:26:13 | INFO | train_inner | epoch 025:   1336 / 11278 loss=7.718, nll_loss=6.416, glat_accu=0, glat_context_p=0.319, word_ins=7.538, length=4.407, ppl=210.51, wps=663.8, ups=1.62, wpb=410.6, bsz=18, num_updates=272000, lr=6.06339e-05, gnorm=0.751, clip=0, loss_scale=32768, train_wall=616, gb_free=13.6, wall=181170
2021-08-22 06:36:42 | INFO | train_inner | epoch 025:   2336 / 11278 loss=7.738, nll_loss=6.435, glat_accu=0, glat_context_p=0.318, word_ins=7.554, length=4.513, ppl=213.54, wps=649.9, ups=1.59, wpb=408.9, bsz=17.6, num_updates=273000, lr=6.05228e-05, gnorm=0.753, clip=0, loss_scale=32768, train_wall=627, gb_free=13.2, wall=181799
2021-08-22 06:47:03 | INFO | train_inner | epoch 025:   3336 / 11278 loss=7.719, nll_loss=6.413, glat_accu=0, glat_context_p=0.318, word_ins=7.536, length=4.471, ppl=210.66, wps=643.7, ups=1.61, wpb=399.2, bsz=17.4, num_updates=274000, lr=6.04122e-05, gnorm=0.767, clip=0, loss_scale=32768, train_wall=618, gb_free=13.1, wall=182419
2021-08-22 06:57:27 | INFO | train_inner | epoch 025:   4336 / 11278 loss=7.737, nll_loss=6.435, glat_accu=0, glat_context_p=0.317, word_ins=7.555, length=4.466, ppl=213.41, wps=650.6, ups=1.6, wpb=406.1, bsz=17.5, num_updates=275000, lr=6.03023e-05, gnorm=0.755, clip=0, loss_scale=32768, train_wall=622, gb_free=12.9, wall=183043
2021-08-22 07:07:48 | INFO | train_inner | epoch 025:   5336 / 11278 loss=7.739, nll_loss=6.44, glat_accu=0, glat_context_p=0.316, word_ins=7.56, length=4.389, ppl=213.66, wps=646.9, ups=1.61, wpb=401.8, bsz=17.4, num_updates=276000, lr=6.01929e-05, gnorm=0.764, clip=0, loss_scale=32768, train_wall=619, gb_free=13.6, wall=183665
2021-08-22 07:18:08 | INFO | train_inner | epoch 025:   6336 / 11278 loss=7.754, nll_loss=6.455, glat_accu=0, glat_context_p=0.316, word_ins=7.572, length=4.43, ppl=215.81, wps=631.6, ups=1.61, wpb=391.9, bsz=16.9, num_updates=277000, lr=6.00842e-05, gnorm=0.774, clip=0, loss_scale=32768, train_wall=618, gb_free=13.5, wall=184285
2021-08-22 07:28:32 | INFO | train_inner | epoch 025:   7336 / 11278 loss=7.734, nll_loss=6.431, glat_accu=0, glat_context_p=0.315, word_ins=7.551, length=4.458, ppl=212.91, wps=638, ups=1.6, wpb=398.2, bsz=17.4, num_updates=278000, lr=5.9976e-05, gnorm=0.763, clip=0, loss_scale=32768, train_wall=622, gb_free=13.4, wall=184909
2021-08-22 07:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-22 07:38:56 | INFO | train_inner | epoch 025:   8337 / 11278 loss=7.724, nll_loss=6.42, glat_accu=0, glat_context_p=0.314, word_ins=7.542, length=4.447, ppl=211.43, wps=656.6, ups=1.6, wpb=409.7, bsz=17.9, num_updates=279000, lr=5.98684e-05, gnorm=0.748, clip=0, loss_scale=32768, train_wall=622, gb_free=13.7, wall=185533
2021-08-22 07:49:16 | INFO | train_inner | epoch 025:   9337 / 11278 loss=7.724, nll_loss=6.419, glat_accu=0, glat_context_p=0.314, word_ins=7.541, length=4.458, ppl=211.41, wps=647.3, ups=1.61, wpb=401, bsz=17.5, num_updates=280000, lr=5.97614e-05, gnorm=0.766, clip=0, loss_scale=32768, train_wall=617, gb_free=13.8, wall=186153
2021-08-22 07:59:11 | INFO | train_inner | epoch 025:  10337 / 11278 loss=7.736, nll_loss=6.435, glat_accu=0, glat_context_p=0.313, word_ins=7.555, length=4.418, ppl=213.14, wps=664.6, ups=1.68, wpb=395.8, bsz=17.2, num_updates=281000, lr=5.9655e-05, gnorm=0.762, clip=0, loss_scale=32768, train_wall=593, gb_free=13.1, wall=186748
2021-08-22 08:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-22 08:16:40 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.228 | nll_loss 6.965 | word_ins 8.01 | length 4.369 | ppl 299.84 | bleu 0 | wps 548.2 | wpb 184.6 | bsz 8 | num_updates 281941 | best_bleu 0
2021-08-22 08:16:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 281941 updates
2021-08-22 08:16:40 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint25.pt
2021-08-22 08:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint25.pt
2021-08-22 08:16:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint25.pt (epoch 25 @ 281941 updates, score 0.0) (writing took 18.002095361007378 seconds)
2021-08-22 08:16:58 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-08-22 08:16:58 | INFO | train | epoch 025 | loss 7.732 | nll_loss 6.429 | glat_accu 0 | glat_context_p 0.316 | word_ins 7.55 | length 4.445 | ppl 212.58 | wps 609.3 | ups 1.51 | wpb 403.7 | bsz 17.5 | num_updates 281941 | lr 5.95554e-05 | gnorm 0.758 | clip 0 | loss_scale 32768 | train_wall 6963 | gb_free 13.4 | wall 187815
2021-08-22 08:16:58 | INFO | fairseq.trainer | begin training epoch 26
2021-08-22 08:16:58 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-22 08:17:32 | INFO | train_inner | epoch 026:     59 / 11278 loss=7.731, nll_loss=6.427, glat_accu=0, glat_context_p=0.312, word_ins=7.548, length=4.462, ppl=212.52, wps=374.2, ups=0.91, wpb=412, bsz=17.8, num_updates=282000, lr=5.95491e-05, gnorm=0.743, clip=0, loss_scale=32768, train_wall=618, gb_free=13.5, wall=187849
2021-08-22 08:27:56 | INFO | train_inner | epoch 026:   1059 / 11278 loss=7.712, nll_loss=6.405, glat_accu=0, glat_context_p=0.312, word_ins=7.528, length=4.464, ppl=209.67, wps=640, ups=1.6, wpb=399.1, bsz=17.4, num_updates=283000, lr=5.94438e-05, gnorm=0.763, clip=0, loss_scale=32768, train_wall=621, gb_free=12.2, wall=188473
2021-08-22 08:38:21 | INFO | train_inner | epoch 026:   2059 / 11278 loss=7.745, nll_loss=6.445, glat_accu=0, glat_context_p=0.311, word_ins=7.564, length=4.403, ppl=214.58, wps=654.1, ups=1.6, wpb=408.5, bsz=17.6, num_updates=284000, lr=5.93391e-05, gnorm=0.766, clip=0, loss_scale=32768, train_wall=622, gb_free=13.1, wall=189097
2021-08-22 08:48:42 | INFO | train_inner | epoch 026:   3059 / 11278 loss=7.704, nll_loss=6.399, glat_accu=0, glat_context_p=0.31, word_ins=7.523, length=4.409, ppl=208.58, wps=638.2, ups=1.61, wpb=396.5, bsz=17.3, num_updates=285000, lr=5.92349e-05, gnorm=0.774, clip=0, loss_scale=32768, train_wall=619, gb_free=13.3, wall=189719
2021-08-22 08:59:03 | INFO | train_inner | epoch 026:   4059 / 11278 loss=7.716, nll_loss=6.41, glat_accu=0, glat_context_p=0.31, word_ins=7.533, length=4.453, ppl=210.3, wps=645.8, ups=1.61, wpb=401.5, bsz=17.5, num_updates=286000, lr=5.91312e-05, gnorm=0.771, clip=0, loss_scale=32768, train_wall=619, gb_free=13.3, wall=190340
2021-08-22 09:09:25 | INFO | train_inner | epoch 026:   5059 / 11278 loss=7.76, nll_loss=6.46, glat_accu=0, glat_context_p=0.309, word_ins=7.577, length=4.449, ppl=216.78, wps=643.6, ups=1.61, wpb=400.3, bsz=17.3, num_updates=287000, lr=5.90281e-05, gnorm=0.773, clip=0, loss_scale=32768, train_wall=619, gb_free=13, wall=190962
2021-08-22 09:19:42 | INFO | train_inner | epoch 026:   6059 / 11278 loss=7.723, nll_loss=6.418, glat_accu=0, glat_context_p=0.308, word_ins=7.54, length=4.453, ppl=211.3, wps=662.5, ups=1.62, wpb=408.3, bsz=17.9, num_updates=288000, lr=5.89256e-05, gnorm=0.763, clip=0, loss_scale=32768, train_wall=614, gb_free=12.9, wall=191579
2021-08-22 09:30:02 | INFO | train_inner | epoch 026:   7059 / 11278 loss=7.742, nll_loss=6.44, glat_accu=0, glat_context_p=0.308, word_ins=7.559, length=4.433, ppl=214.04, wps=654.6, ups=1.61, wpb=406.1, bsz=17.8, num_updates=289000, lr=5.88235e-05, gnorm=0.761, clip=0, loss_scale=32768, train_wall=618, gb_free=13.3, wall=192199
2021-08-22 09:40:23 | INFO | train_inner | epoch 026:   8059 / 11278 loss=7.753, nll_loss=6.45, glat_accu=0, glat_context_p=0.307, word_ins=7.568, length=4.461, ppl=215.7, wps=625.1, ups=1.61, wpb=388.1, bsz=16.8, num_updates=290000, lr=5.8722e-05, gnorm=0.775, clip=0, loss_scale=32768, train_wall=618, gb_free=13.8, wall=192820
2021-08-22 09:50:46 | INFO | train_inner | epoch 026:   9059 / 11278 loss=7.764, nll_loss=6.464, glat_accu=0, glat_context_p=0.306, word_ins=7.581, length=4.44, ppl=217.37, wps=665.1, ups=1.61, wpb=414, bsz=17.6, num_updates=291000, lr=5.8621e-05, gnorm=0.746, clip=0, loss_scale=32768, train_wall=620, gb_free=13.2, wall=193442
2021-08-22 10:01:06 | INFO | train_inner | epoch 026:  10059 / 11278 loss=7.728, nll_loss=6.421, glat_accu=0, glat_context_p=0.306, word_ins=7.543, length=4.468, ppl=211.98, wps=653.9, ups=1.61, wpb=405.9, bsz=17.6, num_updates=292000, lr=5.85206e-05, gnorm=0.766, clip=0, loss_scale=32768, train_wall=618, gb_free=13.2, wall=194063
2021-08-22 10:11:30 | INFO | train_inner | epoch 026:  11059 / 11278 loss=7.724, nll_loss=6.419, glat_accu=0, glat_context_p=0.305, word_ins=7.541, length=4.44, ppl=211.47, wps=653.8, ups=1.6, wpb=407.7, bsz=17.8, num_updates=293000, lr=5.84206e-05, gnorm=0.748, clip=0, loss_scale=32768, train_wall=621, gb_free=12.9, wall=194687
2021-08-22 10:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-22 10:21:36 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.241 | nll_loss 6.991 | word_ins 8.022 | length 4.367 | ppl 302.47 | bleu 0 | wps 545.1 | wpb 184.6 | bsz 8 | num_updates 293219 | best_bleu 0
2021-08-22 10:21:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 293219 updates
2021-08-22 10:21:36 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint26.pt
2021-08-22 10:21:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint26.pt
2021-08-22 10:21:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint26.pt (epoch 26 @ 293219 updates, score 0.0) (writing took 18.15689907001797 seconds)
2021-08-22 10:21:54 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-08-22 10:21:54 | INFO | train | epoch 026 | loss 7.734 | nll_loss 6.431 | glat_accu 0 | glat_context_p 0.308 | word_ins 7.551 | length 4.445 | ppl 212.94 | wps 607.5 | ups 1.5 | wpb 403.7 | bsz 17.5 | num_updates 293219 | lr 5.83988e-05 | gnorm 0.764 | clip 0 | loss_scale 32768 | train_wall 6984 | gb_free 13.3 | wall 195311
2021-08-22 10:21:54 | INFO | fairseq.trainer | begin training epoch 27
2021-08-22 10:21:54 | INFO | fairseq_cli.train | Start iterating over samples
2021-08-22 10:29:58 | INFO | train_inner | epoch 027:    781 / 11278 loss=7.716, nll_loss=6.406, glat_accu=0, glat_context_p=0.304, word_ins=7.529, length=4.514, ppl=210.19, wps=363.8, ups=0.9, wpb=403.1, bsz=17.6, num_updates=294000, lr=5.83212e-05, gnorm=0.78, clip=0, loss_scale=32768, train_wall=622, gb_free=13.7, wall=195795
2021-08-22 10:39:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
2021-08-22 10:40:22 | INFO | train_inner | epoch 027:   1782 / 11278 loss=7.739, nll_loss=6.435, glat_accu=0, glat_context_p=0.304, word_ins=7.556, length=4.447, ppl=213.69, wps=651, ups=1.6, wpb=406.2, bsz=17.6, num_updates=295000, lr=5.82223e-05, gnorm=0.794, clip=0.2, loss_scale=32768, train_wall=622, gb_free=13.8, wall=196419
2021-08-22 10:50:00 | INFO | train_inner | epoch 027:   2782 / 11278 loss=7.73, nll_loss=6.427, glat_accu=0, glat_context_p=0.303, word_ins=7.548, length=4.392, ppl=212.3, wps=697.7, ups=1.73, wpb=403.1, bsz=17.6, num_updates=296000, lr=5.81238e-05, gnorm=0.756, clip=0, loss_scale=32768, train_wall=575, gb_free=13.6, wall=196996
2021-08-22 11:00:18 | INFO | train_inner | epoch 027:   3782 / 11278 loss=7.722, nll_loss=6.416, glat_accu=0, glat_context_p=0.302, word_ins=7.538, length=4.437, ppl=211.14, wps=650.2, ups=1.62, wpb=402.2, bsz=17.5, num_updates=297000, lr=5.80259e-05, gnorm=0.769, clip=0, loss_scale=32768, train_wall=616, gb_free=14.2, wall=197615
2021-08-22 11:10:40 | INFO | train_inner | epoch 027:   4782 / 11278 loss=7.756, nll_loss=6.454, glat_accu=0, glat_context_p=0.302, word_ins=7.572, length=4.442, ppl=216.11, wps=667.3, ups=1.61, wpb=415.3, bsz=18, num_updates=298000, lr=5.79284e-05, gnorm=0.757, clip=0, loss_scale=32768, train_wall=620, gb_free=14.2, wall=198237
2021-08-22 11:21:02 | INFO | train_inner | epoch 027:   5782 / 11278 loss=7.747, nll_loss=6.445, glat_accu=0, glat_context_p=0.301, word_ins=7.564, length=4.418, ppl=214.83, wps=648.6, ups=1.61, wpb=403.4, bsz=17.4, num_updates=299000, lr=5.78315e-05, gnorm=0.778, clip=0, loss_scale=32768, train_wall=620, gb_free=12.4, wall=198859
2021-08-22 11:31:28 | INFO | train_inner | epoch 027:   6782 / 11278 loss=7.738, nll_loss=6.433, glat_accu=0, glat_context_p=0.3, word_ins=7.553, length=4.46, ppl=213.46, wps=657, ups=1.6, wpb=411.2, bsz=17.8, num_updates=300000, lr=5.7735e-05, gnorm=0.763, clip=0, loss_scale=32768, train_wall=624, gb_free=13.9, wall=199485
2021-08-22 11:31:28 | INFO | fairseq_cli.train | Stopping training due to num_updates: 300000 >= max_update: 300000
2021-08-22 11:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-08-22 11:39:12 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.291 | nll_loss 7.059 | word_ins 8.072 | length 4.37 | ppl 313.17 | bleu 0 | wps 546.6 | wpb 184.6 | bsz 8 | num_updates 300000 | best_bleu 0
2021-08-22 11:39:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 300000 updates
2021-08-22 11:39:12 | INFO | fairseq.trainer | Saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint_best.pt
2021-08-22 11:39:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint_best.pt
2021-08-22 11:39:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/v-weipeng/data1/Glat_xsum/save_models/checkpoint_best.pt (epoch 27 @ 300000 updates, score 0.0) (writing took 16.84955949697178 seconds)
2021-08-22 11:39:29 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-08-22 11:39:29 | INFO | train | epoch 027 | loss 7.734 | nll_loss 6.429 | glat_accu 0 | glat_context_p 0.302 | word_ins 7.55 | length 4.444 | ppl 212.85 | wps 590.8 | ups 1.46 | wpb 405.6 | bsz 17.6 | num_updates 300000 | lr 5.7735e-05 | gnorm 0.772 | clip 0 | loss_scale 32768 | train_wall 4158 | gb_free 13.9 | wall 199966
2021-08-22 11:39:29 | INFO | fairseq_cli.train | done training in 199965.9 seconds
